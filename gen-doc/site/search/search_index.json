{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"adgs/","title":"ADGS","text":"<p>Module for interacting with ADGS system through a FastAPI APIRouter.</p> <p>This module provides functionality to retrieve a list of products from the ADGS stations. It includes an API endpoint, utility functions, and initialization for accessing EODataAccessGateway.</p> <p>Module used to download AUX files from ADGS station.</p> <p>HTTP endpoints to get the downloading status from ADGS stations.</p>"},{"location":"adgs/#rs_server_adgs.api.adgs_search.search_products","title":"<code>search_products(request, datetime, limit=1000, sortby='-datetime')</code>","text":"<p>Endpoint to handle the search for products in the AUX station within a specified time interval.</p> <p>This function validates the input 'interval' format, performs a search for products using the ADGS provider, writes the search results to the database, and generates a STAC Feature Collection from the products.</p> Note <ul> <li>The 'interval' parameter should be in ISO 8601 format.</li> <li>The function utilizes the ADGS provider for product search and EODAG for STAC Feature Collection creation.</li> <li>Errors during the process will result in appropriate HTTP status codes and error messages.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>Database</code> <p>The database connection object.</p> required <p>Returns:</p> Type Description <code>list[dict] | dict</code> <p>A list of (or a single) STAC Feature Collection or an error message.</p> <code>list[dict] | dict</code> <p>If no products were found in the mentioned time range, output is an empty list.</p> Source code in <code>rs_server_adgs/api/adgs_search.py</code> <pre><code>@router.get(\"/adgs/aux/search\")\n@apikey_validator(station=\"adgs\", access_type=\"read\")\ndef search_products(  # pylint: disable=too-many-locals\n    request: Request,  # pylint: disable=unused-argument\n    datetime: Annotated[str, Query(description='Time interval e.g. \"2024-01-01T00:00:00Z/2024-01-02T23:59:59Z\"')],\n    limit: Annotated[int, Query(description=\"Maximum number of products to return\")] = 1000,\n    sortby: Annotated[str, Query(description=\"Sort by +/-fieldName (ascending/descending)\")] = \"-datetime\",\n) -&gt; list[dict] | dict:\n    \"\"\"Endpoint to handle the search for products in the AUX station within a specified time interval.\n\n    This function validates the input 'interval' format, performs a search for products using the ADGS provider,\n    writes the search results to the database, and generates a STAC Feature Collection from the products.\n\n    Note:\n        - The 'interval' parameter should be in ISO 8601 format.\n        - The function utilizes the ADGS provider for product search and EODAG for STAC Feature Collection creation.\n        - Errors during the process will result in appropriate HTTP status codes and error messages.\n    \\f\n    Args:\n        db (Database): The database connection object.\n\n    Returns:\n        A list of (or a single) STAC Feature Collection or an error message.\n        If no products were found in the mentioned time range, output is an empty list.\n\n    \"\"\"\n\n    start_date, stop_date = validate_inputs_format(datetime)\n    if limit &lt; 1:\n        raise HTTPException(status_code=status.HTTP_422_UNPROCESSABLE_ENTITY, detail=\"Pagination cannot be less 0\")\n\n    try:\n        time_range = TimeRange(start_date, stop_date)\n        products = init_adgs_provider(\"adgs\").search(time_range, items_per_page=limit)\n        write_search_products_to_db(AdgsDownloadStatus, products)\n        feature_template_path = ADGS_CONFIG / \"ODataToSTAC_template.json\"\n        stac_mapper_path = ADGS_CONFIG / \"adgs_stac_mapper.json\"\n        with (\n            open(feature_template_path, encoding=\"utf-8\") as template,\n            open(stac_mapper_path, encoding=\"utf-8\") as stac_map,\n        ):\n            feature_template = json.loads(template.read())\n            stac_mapper = json.loads(stac_map.read())\n            adgs_item_collection = create_stac_collection(products, feature_template, stac_mapper)\n        logger.info(\"Succesfully listed and processed products from AUX station\")\n        return sort_feature_collection(adgs_item_collection, sortby)\n\n    # pylint: disable=duplicate-code\n    except CreateProviderFailed as exception:\n        logger.error(f\"Failed to create EODAG provider!\\n{traceback.format_exc()}\")\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=f\"Bad station identifier: {exception}\",\n        ) from exception\n\n    # pylint: disable=duplicate-code\n    except sqlalchemy.exc.OperationalError as exception:\n        logger.error(\"Failed to connect to database!\")\n        raise HTTPException(\n            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n            detail=f\"Database connection error: {exception}\",\n        ) from exception\n\n    except requests.exceptions.ConnectionError as exception:\n        logger.error(\"Failed to connect to station!\")\n        raise HTTPException(\n            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n            detail=f\"Station ADGS connection error: {exception}\",\n        ) from exception\n\n    except Exception as exception:  # pylint: disable=broad-exception-caught\n        logger.error(\"General failure!\")\n        raise HTTPException(\n            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n            detail=f\"General failure: {exception}\",\n        ) from exception\n</code></pre>"},{"location":"adgs/#rs_server_adgs.api.adgs_download.AdgsDownloadResponse","title":"<code>AdgsDownloadResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Endpoint response</p> Source code in <code>rs_server_adgs/api/adgs_download.py</code> <pre><code>class AdgsDownloadResponse(BaseModel):\n    \"\"\"Endpoint response\"\"\"\n\n    started: bool\n</code></pre>"},{"location":"adgs/#rs_server_adgs.api.adgs_download.download_products","title":"<code>download_products(request, name, local=None, obs=None, db=Depends(get_db))</code>","text":"<p>Initiate an asynchronous download process for an ADGS product using EODAG.</p> <p>This endpoint triggers the download of an ADGS product identified by the given name of the file. It starts the download process in a separate thread using the start_eodag_download function and updates the product's status in the database. \f</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>Database</code> <p>The database connection object.</p> <code>Depends(get_db)</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary indicating whether the download process has started.</p> Source code in <code>rs_server_adgs/api/adgs_download.py</code> <pre><code>@router.get(\"/adgs/aux\", response_model=AdgsDownloadResponse)\n@apikey_validator(station=\"adgs\", access_type=\"download\")\ndef download_products(\n    request: Request,  # pylint: disable=unused-argument\n    name: Annotated[str, Query(description=\"AUX product name\")],\n    local: Annotated[str | None, Query(description=\"Local download directory\")] = None,\n    obs: Annotated[str | None, Query(description='Object storage path e.g. \"s3://bucket-name/sub/dir\"')] = None,\n    db: Session = Depends(get_db),\n):\n    \"\"\"Initiate an asynchronous download process for an ADGS product using EODAG.\n\n    This endpoint triggers the download of an ADGS product identified by the given\n    name of the file. It starts the download process in a separate thread\n    using the start_eodag_download function and updates the product's status in the database.\n    \\f\n    Args:\n        db (Database): The database connection object.\n\n    Returns:\n        dict: A dictionary indicating whether the download process has started.\n\n    Raises:\n        None\n    \"\"\"\n\n    try:\n        db_product = AdgsDownloadStatus.get(db, name=name)\n    except Exception as exception:  # pylint: disable=broad-exception-caught\n        logger.error(exception)\n        return JSONResponse(\n            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n            content={\"started\": \"false\"},\n        )\n\n    # Reset status to not_started\n    db_product.not_started(db)\n\n    # start a thread to run the action in background\n    thread_started = threading.Event()\n    # fmt: off\n    eodag_args = EoDAGDownloadHandler(\n        AdgsDownloadStatus, thread_started, \"adgs\", str(db_product.product_id),\n        name, local, obs,\n    )\n    # fmt: on\n    thread = threading.Thread(\n        target=start_eodag_download,\n        args=(eodag_args,),\n    )\n    thread.start()\n\n    # check the start of the thread\n    if not thread_started.wait(timeout=DWN_THREAD_START_TIMEOUT):\n        logger.error(\"Download thread did not start !\")\n        # Try n times to update the status to FAILED in the database\n        update_db(db, db_product, EDownloadStatus.FAILED, \"Download thread did not start !\")\n        return JSONResponse(status_code=status.HTTP_408_REQUEST_TIMEOUT, content={\"started\": \"false\"})\n\n    return JSONResponse(status_code=status.HTTP_200_OK, content={\"started\": \"true\"})\n</code></pre>"},{"location":"adgs/#rs_server_adgs.api.adgs_download.start_eodag_download","title":"<code>start_eodag_download(argument)</code>","text":"<p>Start the eodag download process.</p> <p>This function initiates the eodag download process using the provided arguments. It sets up the temporary directory where the files are to be downloaded and gets the database handler</p> <p>Parameters:</p> Name Type Description Default <code>argument</code> <code>EoDAGDownloadHandler</code> <p>An instance of EoDAGDownloadHandler containing the arguments used in the downloading process</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>rs_server_adgs/api/adgs_download.py</code> <pre><code>def start_eodag_download(argument: EoDAGDownloadHandler):\n    \"\"\"Start the eodag download process.\n\n    This function initiates the eodag download process using the provided arguments. It sets up\n    the temporary directory where the files are to be downloaded and gets the database handler\n\n    Args:\n        argument (EoDAGDownloadHandler): An instance of EoDAGDownloadHandler containing\n         the arguments used in the downloading process\n\n    Returns:\n        None\n\n    \"\"\"\n    # Open a database sessions in this thread, because the session from the root thread may have closed.\n    try:\n        with tempfile.TemporaryDirectory() as default_temp_path, contextmanager(get_db)() as db:\n            eodag_download(\n                argument,\n                db,\n                init_adgs_provider,\n                default_path=default_temp_path,\n            )\n    except Exception as e:  # pylint: disable=broad-except\n        logger.error(f\"Exception caught: {e}\")\n</code></pre>"},{"location":"adgs/#rs_server_adgs.api.adgs_status.get_download_status","title":"<code>get_download_status(request, name, db=Depends(get_db))</code>","text":"<p>Get a product download status from its ID or name. \f</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>Session</code> <p>database session</p> <code>Depends(get_db)</code> Source code in <code>rs_server_adgs/api/adgs_status.py</code> <pre><code>@router.get(\"/adgs/aux/status\", response_model=ReadDownloadStatus)\n@apikey_validator(station=\"adgs\", access_type=\"download\")\ndef get_download_status(\n    request: Request,  # pylint: disable=unused-argument\n    name: Annotated[str, Query(description=\"AUX product name\")],\n    db: Session = Depends(get_db),\n):\n    \"\"\"\n    Get a product download status from its ID or name.\n    \\f\n    Args:\n        db (Session): database session\n\n    \"\"\"\n\n    return AdgsDownloadStatus.get(name=name, db=db)\n</code></pre>"},{"location":"cadip/","title":"CADIP","text":"<p>Module for interacting with CADU system through a FastAPI APIRouter.</p> <p>This module provides functionality to retrieve a list of products from the CADU system for a specified station. It includes an API endpoint, utility functions, and initialization for accessing EODataAccessGateway.</p> <p>Module used to download CADU files from CADIP stations.</p> <p>HTTP endpoints to get the downloading status from CADIP stations.</p>"},{"location":"cadip/#rs_server_cadip.api.cadip_search.search_products","title":"<code>search_products(request, datetime, station=FPath(description='CADIP station identifier (MTI, SGS, MPU, INU, etc)'), limit=1000, sortby='-datetime')</code>","text":"<p>Endpoint to retrieve a list of products from the CADU system for a specified station.</p> Notes <ul> <li>The 'interval' parameter should be in ISO 8601 format.</li> <li>The response includes a JSON representation of the list of products for the specified station.</li> <li>In case of an invalid station identifier, a 400 Bad Request response is returned.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>Database</code> <p>The database connection object.</p> required <p>Returns:</p> Type Description <code>list[dict] | dict</code> <p>A list of (or a single) STAC Feature Collection or an error message.</p> <code>list[dict] | dict</code> <p>If the station identifier is invalid, a 400 Bad Request response is returned.</p> <code>list[dict] | dict</code> <p>If no products were found in the mentioned time range, output is an empty list.</p> Source code in <code>rs_server_cadip/api/cadip_search.py</code> <pre><code>@router.get(\"/cadip/{station}/cadu/search\")\n@apikey_validator(station=\"cadip\", access_type=\"read\")\ndef search_products(  # pylint: disable=too-many-locals\n    request: Request,  # pylint: disable=unused-argument\n    datetime: Annotated[str, Query(description='Time interval e.g. \"2024-01-01T00:00:00Z/2024-01-02T23:59:59Z\"')],\n    station: str = FPath(description=\"CADIP station identifier (MTI, SGS, MPU, INU, etc)\"),\n    limit: Annotated[int, Query(description=\"Maximum number of products to return\")] = 1000,\n    sortby: Annotated[str, Query(description=\"Sort by +/-fieldName (ascending/descending)\")] = \"-datetime\",\n) -&gt; list[dict] | dict:\n    \"\"\"Endpoint to retrieve a list of products from the CADU system for a specified station.\n\n    Notes:\n        - The 'interval' parameter should be in ISO 8601 format.\n        - The response includes a JSON representation of the list of products for the specified station.\n        - In case of an invalid station identifier, a 400 Bad Request response is returned.\n    \\f\n    Args:\n        db (Database): The database connection object.\n\n    Returns:\n        A list of (or a single) STAC Feature Collection or an error message.\n        If the station identifier is invalid, a 400 Bad Request response is returned.\n        If no products were found in the mentioned time range, output is an empty list.\n\n    \"\"\"\n\n    start_date, stop_date = validate_inputs_format(datetime)\n    if limit &lt; 1:\n        raise HTTPException(status_code=status.HTTP_422_UNPROCESSABLE_ENTITY, detail=\"Pagination cannot be less 0\")\n    # Init dataretriever / get products / return\n    try:\n        products = init_cadip_provider(station).search(TimeRange(start_date, stop_date), items_per_page=limit)\n        write_search_products_to_db(CadipDownloadStatus, products)\n        feature_template_path = CADIP_CONFIG / \"ODataToSTAC_template.json\"\n        stac_mapper_path = CADIP_CONFIG / \"cadip_stac_mapper.json\"\n        with (\n            open(feature_template_path, encoding=\"utf-8\") as template,\n            open(stac_mapper_path, encoding=\"utf-8\") as stac_map,\n        ):\n            feature_template = json.loads(template.read())\n            stac_mapper = json.loads(stac_map.read())\n            cadip_item_collection = create_stac_collection(products, feature_template, stac_mapper)\n        logger.info(\"Succesfully listed and processed products from CADIP station\")\n        return sort_feature_collection(cadip_item_collection, sortby)\n\n    # pylint: disable=duplicate-code\n    except CreateProviderFailed as exception:\n        logger.error(f\"Failed to create EODAG provider!\\n{traceback.format_exc()}\")\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=f\"Bad station identifier: {exception}\",\n        ) from exception\n\n    # pylint: disable=duplicate-code\n    except sqlalchemy.exc.OperationalError as exception:\n        logger.error(\"Failed to connect to database!\")\n        raise HTTPException(\n            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n            detail=f\"Database connection error: {exception}\",\n        ) from exception\n\n    except requests.exceptions.ConnectionError as exception:\n        logger.error(\"Failed to connect to station!\")\n        raise HTTPException(\n            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n            detail=f\"Station {station} connection error: {exception}\",\n        ) from exception\n\n    except Exception as exception:  # pylint: disable=broad-exception-caught\n        logger.error(\"General failure!\")\n        raise HTTPException(\n            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n            detail=f\"General failure: {exception}\",\n        ) from exception\n</code></pre>"},{"location":"cadip/#rs_server_cadip.api.cadip_search.search_session","title":"<code>search_session(request, station=FPath(description='CADIP station identifier (MTI, SGS, MPU, INU, etc)'), id=None, platform=None, start_date=None, stop_date=None)</code>","text":"<p>Endpoint to retrieve list of sessions from any CADIP station.</p> <p>A valid session search request must contain at least a value for either id or platform or time interval (start_date and stop_date correctly defined).</p> Source code in <code>rs_server_cadip/api/cadip_search.py</code> <pre><code>@router.get(\"/cadip/{station}/session\")\n@apikey_validator(station=\"cadip\", access_type=\"read\")\ndef search_session(\n    request: Request,  # pylint: disable=unused-argument\n    station: str = FPath(description=\"CADIP station identifier (MTI, SGS, MPU, INU, etc)\"),\n    id: Annotated[\n        Union[str, None],\n        Query(\n            description='Session identifier eg: \"S1A_20200105072204051312\" or '\n            '\"S1A_20200105072204051312, S1A_20220715090550123456\"',\n        ),\n    ] = None,\n    platform: Annotated[Union[str, None], Query(description='Satellite identifier eg: \"S1A\" or \"S1A, S1B\"')] = None,\n    start_date: Annotated[Union[str, None], Query(description='Start time e.g. \"2024-01-01T00:00:00Z\"')] = None,\n    stop_date: Annotated[Union[str, None], Query(description='Stop time e.g. \"2024-01-01T00:00:00Z\"')] = None,\n):  # pylint: disable=too-many-arguments, too-many-locals\n    \"\"\"Endpoint to retrieve list of sessions from any CADIP station.\n\n    A valid session search request must contain at least a value for either *id* or *platform* or time interval\n    (*start_date* and *stop_date* correctly defined).\n    \"\"\"\n    session_id: Union[List[str], None] = id.split(\",\") if id else None\n    satellite: Union[List[str], None] = platform.split(\",\") if platform else None\n    time_interval = validate_inputs_format(f\"{start_date}/{stop_date}\") if start_date and stop_date else (None, None)\n\n    if not (session_id or satellite or (time_interval[0] and time_interval[1])):\n        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=\"Missing search parameters\")\n\n    try:\n        products = init_cadip_provider(f\"{station}_session\").search(\n            TimeRange(*time_interval),\n            id=session_id,  # pylint: disable=redefined-builtin\n            platform=satellite,\n            sessions_search=True,\n        )\n        feature_template_path = CADIP_CONFIG / \"cadip_session_ODataToSTAC_template.json\"\n        stac_mapper_path = CADIP_CONFIG / \"cadip_sessions_stac_mapper.json\"\n        with (\n            open(feature_template_path, encoding=\"utf-8\") as template,\n            open(stac_mapper_path, encoding=\"utf-8\") as stac_map,\n        ):\n            feature_template = json.loads(template.read())\n            stac_mapper = json.loads(stac_map.read())\n            cadip_sessions_collection = create_stac_collection(products, feature_template, stac_mapper)\n            return cadip_sessions_collection\n    # except [OSError, FileNotFoundError] as exception:\n    #     return HTTPException(status_code=status.HTTP_503_SERVICE_UNAVAILABLE, detail=f\"Error: {exception}\")\n    except json.JSONDecodeError as exception:\n        raise HTTPException(\n            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n            detail=f\"JSON Map Error: {exception}\",\n        ) from exception\n    except ValueError as exception:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=\"Unable to map OData to STAC.\",\n        ) from exception\n</code></pre>"},{"location":"cadip/#rs_server_cadip.api.cadip_download.CadipDownloadResponse","title":"<code>CadipDownloadResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Endpoint response</p> Source code in <code>rs_server_cadip/api/cadip_download.py</code> <pre><code>class CadipDownloadResponse(BaseModel):\n    \"\"\"Endpoint response\"\"\"\n\n    started: bool\n</code></pre>"},{"location":"cadip/#rs_server_cadip.api.cadip_download.download_products","title":"<code>download_products(request, name, station=FPath(description='CADIP station identifier (MTI, SGS, MPU, INU, etc)'), local=None, obs=None, db=Depends(get_db))</code>","text":"<p>Initiate an asynchronous download process for a CADU product using EODAG.</p> <p>This endpoint triggers the download of a CADU product identified by the given name of the file. It starts the download process in a separate thread using the start_eodag_download function and updates the product's status in the database. \f</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>Database</code> <p>The database connection object.</p> <code>Depends(get_db)</code> Source code in <code>rs_server_cadip/api/cadip_download.py</code> <pre><code>@router.get(\"/cadip/{station}/cadu\", response_model=CadipDownloadResponse)\n@apikey_validator(station=\"cadip\", access_type=\"download\")\ndef download_products(\n    request: Request,  # pylint: disable=unused-argument\n    name: Annotated[str, Query(description=\"CADU product name\")],\n    station: str = FPath(description=\"CADIP station identifier (MTI, SGS, MPU, INU, etc)\"),\n    local: Annotated[str | None, Query(description=\"Local download directory\")] = None,\n    obs: Annotated[str | None, Query(description='Object storage path e.g. \"s3://bucket-name/sub/dir\"')] = None,\n    db: Session = Depends(get_db),\n):  # pylint: disable=too-many-arguments\n    \"\"\"Initiate an asynchronous download process for a CADU product using EODAG.\n\n    This endpoint triggers the download of a CADU product identified by the given\n    name of the file. It starts the download process in a separate thread\n    using the start_eodag_download function and updates the product's status in the database.\n    \\f\n    Args:\n        db (Database): The database connection object.\n    \"\"\"\n\n    # Get the product download status from database\n    try:\n        db_product = CadipDownloadStatus.get(db, name=name)\n    except Exception as exception:  # pylint: disable=broad-exception-caught\n        logger.error(exception)\n        return JSONResponse(\n            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n            content={\"started\": \"false\"},\n        )\n\n    # Reset status to not_started\n    db_product.not_started(db)\n\n    # start a thread to run the action in background\n    logger.debug(\n        \"%s : %s : %s: MAIN THREAD: Starting thread, local = %s\",\n        os.getpid(),\n        threading.get_ident(),\n        datetime.now(),\n        locals(),\n    )\n\n    thread_started = Event()\n    # fmt: off\n    # Skip this function call formatting to avoid the following error: pylint R0801: Similar lines in 2 files\n    eodag_args = EoDAGDownloadHandler(\n        CadipDownloadStatus, thread_started, station.lower(), str(db_product.product_id),\n        name, local, obs,\n    )\n    # fmt: on\n    # Big note / TODO here\n    # Is there a mechanism to catch / capture return value from a function running inside a thread?\n    # If start_eodag_download throws an error, there is no simple solution to return it with FastAPI\n    thread = threading.Thread(\n        target=start_eodag_download,\n        args=(eodag_args,),\n    )\n    thread.start()\n\n    # check the start of the thread\n    if not thread_started.wait(timeout=DWN_THREAD_START_TIMEOUT):\n        logger.error(\"Download thread did not start !\")\n        # Try n times to update the status to FAILED in the database\n        update_db(db, db_product, EDownloadStatus.FAILED, \"Download thread did not start !\")\n        return JSONResponse(status_code=status.HTTP_408_REQUEST_TIMEOUT, content={\"started\": \"false\"})\n\n    return JSONResponse(status_code=status.HTTP_200_OK, content={\"started\": \"true\"})\n</code></pre>"},{"location":"cadip/#rs_server_cadip.api.cadip_download.start_eodag_download","title":"<code>start_eodag_download(argument)</code>","text":"<p>Start the eodag download process.</p> <p>This function initiates the eodag download process using the provided arguments. It sets up the temporary directory where the files are to be downloaded and gets the database handler</p> <p>Parameters:</p> Name Type Description Default <code>argument</code> <code>EoDAGDownloadHandler</code> <p>An instance of EoDAGDownloadHandler containing the arguments used in the downloading process</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>rs_server_cadip/api/cadip_download.py</code> <pre><code>def start_eodag_download(argument: EoDAGDownloadHandler):\n    \"\"\"Start the eodag download process.\n\n    This function initiates the eodag download process using the provided arguments. It sets up\n    the temporary directory where the files are to be downloaded and gets the database handler\n\n    Args:\n        argument (EoDAGDownloadHandler): An instance of EoDAGDownloadHandler containing\n         the arguments used in the downloading process\n\n    Returns:\n        None\n\n    \"\"\"\n\n    # Open a database sessions in this thread, because the session from the root thread may have closed.\n    try:\n        with tempfile.TemporaryDirectory() as default_temp_path, contextmanager(get_db)() as db:\n            eodag_download(\n                argument,\n                db,\n                init_cadip_provider,\n                default_path=default_temp_path,\n            )\n    except Exception as e:  # pylint: disable=broad-except\n        logger.error(f\"Exception caught: {e}\")\n</code></pre>"},{"location":"cadip/#rs_server_cadip.api.cadip_status.get_download_status","title":"<code>get_download_status(request, name, db=Depends(get_db), station=FPath(description='CADIP station identifier (MTI, SGS, MPU, INU, etc)'))</code>","text":"<p>Get a product download status from its ID or name. \f</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>Session</code> <p>database session</p> <code>Depends(get_db)</code> Source code in <code>rs_server_cadip/api/cadip_status.py</code> <pre><code>@router.get(\"/cadip/{station}/cadu/status\", response_model=ReadDownloadStatus)\n@apikey_validator(station=\"cadip\", access_type=\"download\")\ndef get_download_status(\n    request: Request,  # pylint: disable=unused-argument\n    name: Annotated[str, Query(description=\"CADU product name\")],\n    db: Session = Depends(get_db),\n    station: str = FPath(  # pylint: disable=unused-argument\n        description=\"CADIP station identifier (MTI, SGS, MPU, INU, etc)\",\n    ),\n):\n    \"\"\"\n    Get a product download status from its ID or name.\n    \\f\n    Args:\n        db (Session): database session\n    \"\"\"\n\n    return CadipDownloadStatus.get(name=name, db=db)\n</code></pre>"},{"location":"common/","title":"Common","text":"<p>Authentication functions implementation.</p> <p>Note: calls https://gitlab.si.c-s.fr/space_applications/eoservices/apikey-manager</p> <p>Database connection.</p> <p>Taken from: https://praciano.com.br/fastapi-and-async-sqlalchemy-20-with-pytest-done-right.html</p> <p>Logging utility.</p> <p>OpenTelemetry utility</p> <p>This module is used to share common functions between apis endpoints</p> <p>EODAG Provider.</p> <p>Provider mechanism.</p> <p>TODO Docstring to be added.</p>"},{"location":"common/#rs_server_common.authentication.__apikey_security_cached","title":"<code>__apikey_security_cached(apikey_value)</code>  <code>async</code>","text":"<p>Cached version of apikey_security. Cache an infinite (sys.maxsize) number of results for 120 seconds.</p> Source code in <code>rs_server_common/authentication.py</code> <pre><code>@cached(cache=ttl_cache)\nasync def __apikey_security_cached(apikey_value) -&gt; tuple[list, dict, dict]:\n    \"\"\"\n    Cached version of apikey_security. Cache an infinite (sys.maxsize) number of results for 120 seconds.\n    \"\"\"\n    # The uac manager check url is passed as an environment variable\n    try:\n        check_url = env[\"RSPY_UAC_CHECK_URL\"]\n    except KeyError:\n        raise HTTPException(HTTP_400_BAD_REQUEST, \"UAC manager URL is undefined\")  # pylint: disable=raise-missing-from\n\n    # Request the uac, pass user-defined api key in http header\n    try:\n        response = await settings.http_client().get(check_url, headers={APIKEY_HEADER: apikey_value or \"\"})\n    except httpx.HTTPError as error:\n        message = \"Error connecting to the UAC manager\"\n        logger.error(f\"{message}\\n{traceback.format_exc()}\")\n        raise HTTPException(HTTP_500_INTERNAL_SERVER_ERROR, message) from error\n\n    # Read the api key info\n    if response.is_success:\n        contents = response.json()\n        # Note: for now, config is an empty dict\n        return contents[\"iam_roles\"], contents[\"config\"], contents[\"user_login\"]\n\n    # Try to read the response detail or error\n    try:\n        json = response.json()\n        if \"detail\" in json:\n            detail = json[\"detail\"]\n        else:\n            detail = json[\"error\"]\n\n    # If this fail, get the full response content\n    except Exception:  # pylint: disable=broad-exception-caught\n        detail = response.read().decode(\"utf-8\")\n\n    # Forward error\n    raise HTTPException(response.status_code, f\"UAC manager: {detail}\")\n</code></pre>"},{"location":"common/#rs_server_common.authentication.apikey_security","title":"<code>apikey_security(request, apikey_header)</code>  <code>async</code>","text":"<p>FastAPI Security dependency for the cluster mode. Check the api key validity, passed as an HTTP header.</p> <p>Parameters:</p> Name Type Description Default <code>apikey_header</code> <code>Security</code> <p>API key passed in HTTP header</p> required <p>Returns:</p> Type Description <code>tuple[list, dict, str]</code> <p>Tuple of (IAM roles, config) information from the keycloak server, associated with the api key.</p> Source code in <code>rs_server_common/authentication.py</code> <pre><code>async def apikey_security(\n    request: Request,\n    apikey_header: Annotated[str, Security(APIKEY_AUTH_HEADER)],\n    # apikey_query: Annotated[str, Security(APIKEY_AUTH_QUERY)],\n) -&gt; tuple[list, dict, str]:\n    \"\"\"\n    FastAPI Security dependency for the cluster mode. Check the api key validity, passed as an HTTP header.\n\n    Args:\n        apikey_header (Security): API key passed in HTTP header\n\n    Returns:\n        Tuple of (IAM roles, config) information from the keycloak server, associated with the api key.\n    \"\"\"\n\n    # Use the api key passed by either http headers or query parameter (disabled for now)\n    apikey_value = apikey_header  # or apikey_query\n    if not apikey_value:\n        raise HTTPException(\n            status_code=HTTP_403_FORBIDDEN,\n            detail=\"Not authenticated\",\n        )\n\n    # Call the cached function (fastapi Depends doesn't work with @cached)\n    auth_roles, auth_config, user_login = await __apikey_security_cached(str(apikey_value))\n    request.state.auth_roles = auth_roles\n    request.state.auth_config = auth_config\n    request.state.user_login = user_login\n    logger.debug(f\"API key information: {auth_roles, auth_config, user_login}\")\n    return auth_roles, auth_config, user_login\n</code></pre>"},{"location":"common/#rs_server_common.authentication.apikey_validator","title":"<code>apikey_validator(station, access_type)</code>","text":"<p>Decorator to validate API key access.</p> <p>Parameters:</p> Name Type Description Default <code>station</code> <code>str</code> <p>The station name = adgs or cadip</p> required <code>access_type</code> <code>str</code> <p>The type of access.</p> required <p>Raises:</p> Type Description <code>HTTPException</code> <p>If the authorization key does not include the right role to access the specified station.</p> <p>Returns:</p> Name Type Description <code>function</code> <p>Decorator function.</p> Source code in <code>rs_server_common/authentication.py</code> <pre><code>def apikey_validator(station, access_type):\n    \"\"\"Decorator to validate API key access.\n\n    Args:\n        station (str): The station name = adgs or cadip\n        access_type (str): The type of access.\n\n    Raises:\n        HTTPException: If the authorization key does not include the right role\n            to access the specified station.\n\n    Returns:\n        function: Decorator function.\n    \"\"\"\n\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            if settings.CLUSTER_MODE:\n                # Read the full cadip station passed in parameter e.g. INS, MPS, ...\n                if station == \"cadip\":\n                    cadip_station = kwargs[\"station\"]  # ins, mps, mti, nsg, sgs, or cadip\n                    try:\n                        full_station = STATIONS_AUTH_LUT[cadip_station.lower()]\n                    except KeyError as exception:\n                        raise HTTPException(\n                            status_code=status.HTTP_400_BAD_REQUEST,\n                            detail=f\"Unknown CADIP station: {cadip_station!r}\",\n                        ) from exception\n                else:  # for adgs\n                    full_station = station\n\n                requested_role = f\"rs_{full_station}_{access_type}\".upper()\n                try:\n                    auth_roles = [role.upper() for role in kwargs[\"request\"].state.auth_roles]\n                except KeyError:\n                    auth_roles = []\n\n                if requested_role not in auth_roles:\n                    raise HTTPException(\n                        status_code=status.HTTP_401_UNAUTHORIZED,\n                        detail=f\"Authorization key does not include the right role to {access_type} \"\n                        f\"from the {full_station!r} station\",\n                    )\n\n            return func(*args, **kwargs)\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"common/#rs_server_common.db.database.DatabaseSessionManager","title":"<code>DatabaseSessionManager</code>","text":"<p>Database session configuration.</p> Source code in <code>rs_server_common/db/database.py</code> <pre><code>class DatabaseSessionManager:\n    \"\"\"Database session configuration.\"\"\"\n\n    lock = Lock()\n    multiprocessing_lock = multiprocessing.Lock()\n\n    def __init__(self):\n        \"\"\"Create a Database session configuration.\"\"\"\n        self._engine: Engine | None = None\n        self._sessionmaker: sessionmaker | None = None\n\n    @classmethod\n    def url(cls):\n        \"\"\"Get database connection URL.\"\"\"\n        try:\n            # pylint: disable=consider-using-f-string\n            return os.getenv(\n                \"POSTGRES_URL\",\n                \"postgresql+psycopg2://{user}:{password}@{host}:{port}/{dbname}\".format(\n                    user=os.environ[\"POSTGRES_USER\"],\n                    password=os.environ[\"POSTGRES_PASSWORD\"],\n                    host=os.environ[\"POSTGRES_HOST\"],\n                    port=os.environ[\"POSTGRES_PORT\"],\n                    dbname=os.environ[\"POSTGRES_DB\"],\n                ),\n            )\n        except KeyError as key_error:\n            raise KeyError(\n                \"The PostgreSQL environment variables are missing: \"\n                \"POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_HOST, POSTGRES_PORT, POSTGRES_DB\",\n            ) from key_error\n\n    def open_session(self, url: str = \"\"):\n        \"\"\"Open database session.\"\"\"\n\n        # If the 'self' object is used by several threads in the same process,\n        # make sure to initialize the session only once.\n        with DatabaseSessionManager.lock:\n            if (self._engine is None) or (self._sessionmaker is None):\n                self._engine = create_engine(url or self.url(), poolclass=NullPool, pool_pre_ping=True)\n                self._sessionmaker = sessionmaker(autocommit=False, autoflush=False, bind=self._engine)\n\n                try:\n                    # Create all tables.\n                    # Warning: this only works if the database table modules have been imported\n                    # e.g. import rs_server_adgs.adgs_download_status\n                    self.create_all()\n\n                # It fails if the database is unreachable, but even in this case the engine and session are not None.\n                # Set them to None so we will try to create all tables again on the next try.\n                except Exception:\n                    self.close()\n                    raise\n\n    def close(self):\n        \"\"\"Close database session.\"\"\"\n        if self._engine is not None:\n            self._engine.dispose()\n            self._engine = None\n        self._sessionmaker = None\n\n    @contextlib.contextmanager\n    def connect(self) -&gt; Iterator[Connection]:\n        \"\"\"Open new database connection instance.\"\"\"\n\n        if self._engine is None:\n            raise RuntimeError(\"DatabaseSessionManager is not initialized\")\n\n        with self._engine.begin() as connection:\n            try:\n                yield connection\n\n            # In case of any exception, rollback connection and re-raise into HTTP exception\n            except Exception as exception:  # pylint: disable=broad-exception-caught\n                connection.rollback()\n                self.reraise_http_exception(exception)\n\n    @contextlib.contextmanager\n    def session(self) -&gt; Iterator[Session]:\n        \"\"\"Open new database session instance.\"\"\"\n\n        if self._sessionmaker is None:\n            raise RuntimeError(\"DatabaseSessionManager is not initialized\")\n\n        session = self._sessionmaker()\n        try:\n            yield session\n\n        # In case of any exception, rollback session and re-raise into HTTP exception\n        except Exception as exception:  # pylint: disable=broad-exception-caught\n            session.rollback()\n            self.reraise_http_exception(exception)\n\n        # Close session when deleting instance.\n        finally:\n            session.close()\n\n    @staticmethod\n    def __filelock(func):\n        \"\"\"Avoid concurrent writing to the database using a file locK.\"\"\"\n\n        @wraps(func)\n        def with_filelock(*args, **kwargs):\n            \"\"\"Wrap the the call to 'func' inside the lock.\"\"\"\n\n            # Let's do this only if the RSPY_WORKING_DIR environment variable is defined.\n            # Write a .lock file inside this directory.\n            try:\n                with FileLock(Path(os.environ[\"RSPY_WORKING_DIR\"]) / f\"{__name__}.lock\"):\n                    return func(*args, **kwargs)\n\n            # Else just call the function without a lock\n            except KeyError:\n                return func(*args, **kwargs)\n\n        return with_filelock\n\n    @__filelock\n    def create_all(self):\n        \"\"\"Create all database tables.\"\"\"\n        with DatabaseSessionManager.multiprocessing_lock:  # Handle concurrent table creation by different processes\n            Base.metadata.create_all(bind=self._engine)\n\n    @__filelock\n    def drop_all(self):\n        \"\"\"Drop all database tables.\"\"\"\n        with DatabaseSessionManager.multiprocessing_lock:  # Handle concurrent table creation by different processes\n            Base.metadata.drop_all(bind=self._engine)\n\n    @classmethod\n    def reraise_http_exception(cls, exception: Exception):\n        \"\"\"Re-raise all exceptions into HTTP exceptions.\"\"\"\n\n        # Raised exceptions are not always printed in the console, so do it manually with the stacktrace.\n        logger.error(traceback.format_exc())\n\n        if isinstance(exception, StarletteHTTPException):\n            raise exception\n        raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=repr(exception))\n</code></pre>"},{"location":"common/#rs_server_common.db.database.DatabaseSessionManager.__filelock","title":"<code>__filelock(func)</code>  <code>staticmethod</code>","text":"<p>Avoid concurrent writing to the database using a file locK.</p> Source code in <code>rs_server_common/db/database.py</code> <pre><code>@staticmethod\ndef __filelock(func):\n    \"\"\"Avoid concurrent writing to the database using a file locK.\"\"\"\n\n    @wraps(func)\n    def with_filelock(*args, **kwargs):\n        \"\"\"Wrap the the call to 'func' inside the lock.\"\"\"\n\n        # Let's do this only if the RSPY_WORKING_DIR environment variable is defined.\n        # Write a .lock file inside this directory.\n        try:\n            with FileLock(Path(os.environ[\"RSPY_WORKING_DIR\"]) / f\"{__name__}.lock\"):\n                return func(*args, **kwargs)\n\n        # Else just call the function without a lock\n        except KeyError:\n            return func(*args, **kwargs)\n\n    return with_filelock\n</code></pre>"},{"location":"common/#rs_server_common.db.database.DatabaseSessionManager.__init__","title":"<code>__init__()</code>","text":"<p>Create a Database session configuration.</p> Source code in <code>rs_server_common/db/database.py</code> <pre><code>def __init__(self):\n    \"\"\"Create a Database session configuration.\"\"\"\n    self._engine: Engine | None = None\n    self._sessionmaker: sessionmaker | None = None\n</code></pre>"},{"location":"common/#rs_server_common.db.database.DatabaseSessionManager.close","title":"<code>close()</code>","text":"<p>Close database session.</p> Source code in <code>rs_server_common/db/database.py</code> <pre><code>def close(self):\n    \"\"\"Close database session.\"\"\"\n    if self._engine is not None:\n        self._engine.dispose()\n        self._engine = None\n    self._sessionmaker = None\n</code></pre>"},{"location":"common/#rs_server_common.db.database.DatabaseSessionManager.connect","title":"<code>connect()</code>","text":"<p>Open new database connection instance.</p> Source code in <code>rs_server_common/db/database.py</code> <pre><code>@contextlib.contextmanager\ndef connect(self) -&gt; Iterator[Connection]:\n    \"\"\"Open new database connection instance.\"\"\"\n\n    if self._engine is None:\n        raise RuntimeError(\"DatabaseSessionManager is not initialized\")\n\n    with self._engine.begin() as connection:\n        try:\n            yield connection\n\n        # In case of any exception, rollback connection and re-raise into HTTP exception\n        except Exception as exception:  # pylint: disable=broad-exception-caught\n            connection.rollback()\n            self.reraise_http_exception(exception)\n</code></pre>"},{"location":"common/#rs_server_common.db.database.DatabaseSessionManager.create_all","title":"<code>create_all()</code>","text":"<p>Create all database tables.</p> Source code in <code>rs_server_common/db/database.py</code> <pre><code>@__filelock\ndef create_all(self):\n    \"\"\"Create all database tables.\"\"\"\n    with DatabaseSessionManager.multiprocessing_lock:  # Handle concurrent table creation by different processes\n        Base.metadata.create_all(bind=self._engine)\n</code></pre>"},{"location":"common/#rs_server_common.db.database.DatabaseSessionManager.drop_all","title":"<code>drop_all()</code>","text":"<p>Drop all database tables.</p> Source code in <code>rs_server_common/db/database.py</code> <pre><code>@__filelock\ndef drop_all(self):\n    \"\"\"Drop all database tables.\"\"\"\n    with DatabaseSessionManager.multiprocessing_lock:  # Handle concurrent table creation by different processes\n        Base.metadata.drop_all(bind=self._engine)\n</code></pre>"},{"location":"common/#rs_server_common.db.database.DatabaseSessionManager.open_session","title":"<code>open_session(url='')</code>","text":"<p>Open database session.</p> Source code in <code>rs_server_common/db/database.py</code> <pre><code>def open_session(self, url: str = \"\"):\n    \"\"\"Open database session.\"\"\"\n\n    # If the 'self' object is used by several threads in the same process,\n    # make sure to initialize the session only once.\n    with DatabaseSessionManager.lock:\n        if (self._engine is None) or (self._sessionmaker is None):\n            self._engine = create_engine(url or self.url(), poolclass=NullPool, pool_pre_ping=True)\n            self._sessionmaker = sessionmaker(autocommit=False, autoflush=False, bind=self._engine)\n\n            try:\n                # Create all tables.\n                # Warning: this only works if the database table modules have been imported\n                # e.g. import rs_server_adgs.adgs_download_status\n                self.create_all()\n\n            # It fails if the database is unreachable, but even in this case the engine and session are not None.\n            # Set them to None so we will try to create all tables again on the next try.\n            except Exception:\n                self.close()\n                raise\n</code></pre>"},{"location":"common/#rs_server_common.db.database.DatabaseSessionManager.reraise_http_exception","title":"<code>reraise_http_exception(exception)</code>  <code>classmethod</code>","text":"<p>Re-raise all exceptions into HTTP exceptions.</p> Source code in <code>rs_server_common/db/database.py</code> <pre><code>@classmethod\ndef reraise_http_exception(cls, exception: Exception):\n    \"\"\"Re-raise all exceptions into HTTP exceptions.\"\"\"\n\n    # Raised exceptions are not always printed in the console, so do it manually with the stacktrace.\n    logger.error(traceback.format_exc())\n\n    if isinstance(exception, StarletteHTTPException):\n        raise exception\n    raise HTTPException(status_code=HTTP_500_INTERNAL_SERVER_ERROR, detail=repr(exception))\n</code></pre>"},{"location":"common/#rs_server_common.db.database.DatabaseSessionManager.session","title":"<code>session()</code>","text":"<p>Open new database session instance.</p> Source code in <code>rs_server_common/db/database.py</code> <pre><code>@contextlib.contextmanager\ndef session(self) -&gt; Iterator[Session]:\n    \"\"\"Open new database session instance.\"\"\"\n\n    if self._sessionmaker is None:\n        raise RuntimeError(\"DatabaseSessionManager is not initialized\")\n\n    session = self._sessionmaker()\n    try:\n        yield session\n\n    # In case of any exception, rollback session and re-raise into HTTP exception\n    except Exception as exception:  # pylint: disable=broad-exception-caught\n        session.rollback()\n        self.reraise_http_exception(exception)\n\n    # Close session when deleting instance.\n    finally:\n        session.close()\n</code></pre>"},{"location":"common/#rs_server_common.db.database.DatabaseSessionManager.url","title":"<code>url()</code>  <code>classmethod</code>","text":"<p>Get database connection URL.</p> Source code in <code>rs_server_common/db/database.py</code> <pre><code>@classmethod\ndef url(cls):\n    \"\"\"Get database connection URL.\"\"\"\n    try:\n        # pylint: disable=consider-using-f-string\n        return os.getenv(\n            \"POSTGRES_URL\",\n            \"postgresql+psycopg2://{user}:{password}@{host}:{port}/{dbname}\".format(\n                user=os.environ[\"POSTGRES_USER\"],\n                password=os.environ[\"POSTGRES_PASSWORD\"],\n                host=os.environ[\"POSTGRES_HOST\"],\n                port=os.environ[\"POSTGRES_PORT\"],\n                dbname=os.environ[\"POSTGRES_DB\"],\n            ),\n        )\n    except KeyError as key_error:\n        raise KeyError(\n            \"The PostgreSQL environment variables are missing: \"\n            \"POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_HOST, POSTGRES_PORT, POSTGRES_DB\",\n        ) from key_error\n</code></pre>"},{"location":"common/#rs_server_common.db.database.get_db","title":"<code>get_db()</code>","text":"<p>Return a database session for FastAPI dependency injection.</p> Source code in <code>rs_server_common/db/database.py</code> <pre><code>def get_db():\n    \"\"\"Return a database session for FastAPI dependency injection.\"\"\"\n    try:\n        with sessionmanager.session() as session:\n            yield session\n\n    # Re-raise all exceptions into HTTP exceptions\n    except Exception as exception:  # pylint: disable=broad-exception-caught\n        DatabaseSessionManager.reraise_http_exception(exception)\n</code></pre>"},{"location":"common/#rs_server_common.utils.logging.CustomFormatter","title":"<code>CustomFormatter</code>","text":"<p>               Bases: <code>Formatter</code></p> <p>Custom logging formatter with colored text. See: https://stackoverflow.com/a/56944256</p> Source code in <code>rs_server_common/utils/logging.py</code> <pre><code>class CustomFormatter(logging.Formatter):\n    \"\"\"\n    Custom logging formatter with colored text.\n    See: https://stackoverflow.com/a/56944256\n    \"\"\"\n\n    _RED = \"\\x1b[31m\"\n    _BOLD_RED = \"\\x1b[31;1m\"\n    _GREEN = \"\\x1b[32m\"\n    _YELLOW = \"\\x1b[33m\"\n    _PURPLE = \"\\x1b[35m\"\n    _RESET = \"\\x1b[0m\"\n\n    _FORMAT = f\"%(asctime)s.%(msecs)03d [{{color}}%(levelname)s{_RESET}] (%(name)s) %(message)s\"\n    _DATETIME = \"%H:%M:%S\"\n\n    _FORMATS = {\n        logging.NOTSET: _FORMAT.format(color=\"\"),\n        logging.DEBUG: _FORMAT.format(color=_PURPLE),\n        logging.INFO: _FORMAT.format(color=_GREEN),\n        logging.WARNING: _FORMAT.format(color=_YELLOW),\n        logging.ERROR: _FORMAT.format(color=_BOLD_RED),\n        logging.CRITICAL: _FORMAT.format(color=_RED),\n    }\n\n    def format(self, record):\n        level_format = self._FORMATS.get(record.levelno)\n        formatter = logging.Formatter(level_format, self._DATETIME)\n        return formatter.format(record)\n</code></pre>"},{"location":"common/#rs_server_common.utils.logging.Logging","title":"<code>Logging</code>","text":"<p>Logging utility.</p> <p>Attributes:</p> Name Type Description <code>lock</code> <p>For code synchronization</p> <code>level</code> <p>Minimal log level to use for all new logging instances.</p> Source code in <code>rs_server_common/utils/logging.py</code> <pre><code>class Logging:  # pylint: disable=too-few-public-methods\n    \"\"\"\n    Logging utility.\n\n    Attributes:\n        lock: For code synchronization\n        level: Minimal log level to use for all new logging instances.\n    \"\"\"\n\n    lock = Lock()\n    level = logging.DEBUG\n\n    @classmethod\n    def default(cls, name=\"rspy\"):\n        \"\"\"\n        Return a default Logger class instance.\n\n        Args:\n            name: Logger name. You can pass __name__ to use your current module name.\n        \"\"\"\n        logger = logging.getLogger(name=name)\n\n        with cls.lock:\n            # Don't propagate to root logger\n            logger.propagate = False\n\n            # If we have already set the handlers for the logger with this name, do nothing more\n            if logger.hasHandlers():\n                return logger\n\n            # Set the minimal log level to use for all new logging instances.\n            logger.setLevel(cls.level)\n\n            # Create console handler\n            handler = logging.StreamHandler()\n            handler.setFormatter(CustomFormatter())\n            logger.addHandler(handler)\n\n            # Export logs to Loki, see: https://pypi.org/project/python-logging-loki/\n            # Note: on the cluster, this is not used. Promtail already forwards stdout to Loki.\n            loki_endpoint = os.getenv(\"LOKI_ENDPOINT\")\n            if loki_endpoint and settings.SERVICE_NAME:\n                handler = logging_loki.LokiQueueHandler(\n                    Queue(-1),\n                    url=loki_endpoint,\n                    tags={\"service\": settings.SERVICE_NAME},\n                    # auth=(\"username\", \"password\"),\n                    version=\"1\",\n                )\n                handler.setFormatter(CustomFormatter())\n                logger.addHandler(handler)\n\n            return logger\n</code></pre>"},{"location":"common/#rs_server_common.utils.logging.Logging.default","title":"<code>default(name='rspy')</code>  <code>classmethod</code>","text":"<p>Return a default Logger class instance.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <p>Logger name. You can pass name to use your current module name.</p> <code>'rspy'</code> Source code in <code>rs_server_common/utils/logging.py</code> <pre><code>@classmethod\ndef default(cls, name=\"rspy\"):\n    \"\"\"\n    Return a default Logger class instance.\n\n    Args:\n        name: Logger name. You can pass __name__ to use your current module name.\n    \"\"\"\n    logger = logging.getLogger(name=name)\n\n    with cls.lock:\n        # Don't propagate to root logger\n        logger.propagate = False\n\n        # If we have already set the handlers for the logger with this name, do nothing more\n        if logger.hasHandlers():\n            return logger\n\n        # Set the minimal log level to use for all new logging instances.\n        logger.setLevel(cls.level)\n\n        # Create console handler\n        handler = logging.StreamHandler()\n        handler.setFormatter(CustomFormatter())\n        logger.addHandler(handler)\n\n        # Export logs to Loki, see: https://pypi.org/project/python-logging-loki/\n        # Note: on the cluster, this is not used. Promtail already forwards stdout to Loki.\n        loki_endpoint = os.getenv(\"LOKI_ENDPOINT\")\n        if loki_endpoint and settings.SERVICE_NAME:\n            handler = logging_loki.LokiQueueHandler(\n                Queue(-1),\n                url=loki_endpoint,\n                tags={\"service\": settings.SERVICE_NAME},\n                # auth=(\"username\", \"password\"),\n                version=\"1\",\n            )\n            handler.setFormatter(CustomFormatter())\n            logger.addHandler(handler)\n\n        return logger\n</code></pre>"},{"location":"common/#rs_server_common.utils.opentelemetry.init_traces","title":"<code>init_traces(app, service_name)</code>","text":"<p>Init instrumentation of OpenTelemetry traces.</p> <p>Parameters:</p> Name Type Description Default <code>app</code> <code>FastAPI</code> <p>FastAPI application</p> required <code>service_name</code> <code>str</code> <p>service name</p> required Source code in <code>rs_server_common/utils/opentelemetry.py</code> <pre><code>def init_traces(app: fastapi.FastAPI, service_name: str):\n    \"\"\"\n    Init instrumentation of OpenTelemetry traces.\n\n    Args:\n        app (fastapi.FastAPI): FastAPI application\n        service_name (str): service name\n    \"\"\"\n\n    # See: https://github.com/softwarebloat/python-tracing-demo/tree/main\n\n    tempo_endpoint = os.getenv(\"TEMPO_ENDPOINT\")\n    if not tempo_endpoint:\n        return\n\n    otel_resource = Resource(attributes={\"service.name\": service_name})\n    otel_tracer = TracerProvider(resource=otel_resource)\n    trace.set_tracer_provider(otel_tracer)\n    otel_tracer.add_span_processor(BatchSpanProcessor(OTLPSpanExporter(endpoint=tempo_endpoint)))\n\n    FastAPIInstrumentor.instrument_app(app, tracer_provider=otel_tracer)\n    # logger.debug(f\"OpenTelemetry instrumentation of 'fastapi.FastAPIInstrumentor'\")\n\n    # Instrument all the dependencies under opentelemetry.instrumentation.*\n    # NOTE: we need 'poetry run opentelemetry-bootstrap -a install' to install these.\n\n    package = opentelemetry.instrumentation\n    prefix = package.__name__ + \".\"\n    classes = set()\n\n    # We need an empty PYTHONPATH if the env var is missing\n    os.environ[\"PYTHONPATH\"] = os.getenv(\"PYTHONPATH\", \"\")\n\n    # Recursively find all package modules\n    for _, module_str, _ in pkgutil.walk_packages(path=package.__path__, prefix=prefix, onerror=None):\n\n        # Don't instrument these modules, they have errors, maybe we should see why\n        if module_str in [\"opentelemetry.instrumentation.tortoiseorm\"]:\n            continue\n\n        # Import and find all module classes\n        __import__(module_str)\n        for _, _class in inspect.getmembers(sys.modules[module_str]):\n            if (not inspect.isclass(_class)) or (_class in classes):\n                continue\n\n            # Save the class (classes are found several times when imported by other modules)\n            classes.add(_class)\n\n            # Don't instrument these classes, they have errors, maybe we should see why\n            if _class in [AsyncioInstrumentor, AwsLambdaInstrumentor, BaseInstrumentor, FastAPIInstrumentor]:\n                continue\n\n            # If the \"instrument\" method exists, call it\n            _instrument = getattr(_class, \"instrument\", None)\n            if callable(_instrument):\n\n                _class_instance = _class()\n                if not _class_instance.is_instrumented_by_opentelemetry:\n                    _class_instance.instrument(tracer_provider=otel_tracer)\n</code></pre>"},{"location":"common/#rs_server_common.utils.utils.EoDAGDownloadHandler","title":"<code>EoDAGDownloadHandler</code>  <code>dataclass</code>","text":"<p>Dataclass to store arguments needed for eodag download.</p> <p>Attributes:</p> Name Type Description <code>db_handler</code> <code>DownloadStatus</code> <p>An instance used to access the database.</p> <code>thread_started</code> <code>Event</code> <p>Event to signal the start of the download thread.</p> <code>station</code> <code>str</code> <p>Station identifier (needed only for CADIP).</p> <code>product_id</code> <code>str</code> <p>Identifier of the product to be downloaded.</p> <code>name</code> <code>str</code> <p>Filename of the file to be downloaded.</p> <code>local</code> <code>str | None</code> <p>Local path where the product will be stored</p> <code>obs</code> <code>str | None</code> <p>Path to the S3 storage where the file will be uploaded</p> Source code in <code>rs_server_common/utils/utils.py</code> <pre><code>@dataclass\nclass EoDAGDownloadHandler:\n    \"\"\"Dataclass to store arguments needed for eodag download.\n\n    Attributes:\n        db_handler (DownloadStatus): An instance used to access the database.\n        thread_started (threading.Event): Event to signal the start of the download thread.\n        station (str): Station identifier (needed only for CADIP).\n        product_id (str): Identifier of the product to be downloaded.\n        name (str): Filename of the file to be downloaded.\n        local (str | None): Local path where the product will be stored\n        obs (str | None): Path to the S3 storage where the file will be uploaded\n    \"\"\"\n\n    db_handler: DownloadStatus\n    thread_started: threading.Event\n    station: str  # needed only for CADIP\n    product_id: str\n    name: str\n    local: str | None\n    obs: str | None\n</code></pre>"},{"location":"common/#rs_server_common.utils.utils.create_stac_collection","title":"<code>create_stac_collection(products, feature_template, stac_mapper)</code>","text":"<p>This function create a STAC feature for each EOProduct based on a given template</p> Source code in <code>rs_server_common/utils/utils.py</code> <pre><code>def create_stac_collection(products: List[EOProduct], feature_template: dict, stac_mapper: dict) -&gt; dict:\n    \"\"\"This function create a STAC feature for each EOProduct based on a given template\"\"\"\n    stac_template: Dict[Any, Any] = {\n        \"type\": \"FeatureCollection\",\n        \"numberMatched\": 0,\n        \"numberReturned\": 0,\n        \"features\": [],\n    }\n    for product in products:\n        product_data = extract_eo_product(product, stac_mapper)\n        feature_tmp = odata_to_stac(copy.deepcopy(feature_template), product_data, stac_mapper)\n        stac_template[\"numberMatched\"] += 1\n        stac_template[\"numberReturned\"] += 1\n        stac_template[\"features\"].append(feature_tmp)\n    return stac_template\n</code></pre>"},{"location":"common/#rs_server_common.utils.utils.eodag_download","title":"<code>eodag_download(argument, db, init_provider, **kwargs)</code>","text":"<p>Start the eodag download process.</p> <p>This function initiates the eodag download process using the provided arguments. It sets up the necessary configurations, starts the download thread, and updates the download status in the database based on the outcome of the download.</p> <p>Parameters:</p> Name Type Description Default <code>argument</code> <code>EoDAGDownloadHandler</code> <p>An instance of EoDAGDownloadHandler containing the arguments used in the downloading process</p> required Note <p>The local and obs parameters are optionals: - local (str | None): Local path where the product will be stored. If this     parameter is not given, the local path where the file is stored will be set to a temporary one - obs (str | None): Path to S3 storage where the file will be uploaded, after a successfull download from CADIP     server. If this parameter is not given, the file will not be uploaded to the s3 storage.</p> <p>Returns:</p> Type Description <p>None</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If there is an issue connecting to the S3 storage during the download.</p> Source code in <code>rs_server_common/utils/utils.py</code> <pre><code>def eodag_download(argument: EoDAGDownloadHandler, db, init_provider: Callable[[str], Provider], **kwargs):\n    \"\"\"Start the eodag download process.\n\n    This function initiates the eodag download process using the provided arguments. It sets up\n    the necessary configurations, starts the download thread, and updates the download status in the\n    database based on the outcome of the download.\n\n    Args:\n        argument (EoDAGDownloadHandler): An instance of EoDAGDownloadHandler containing\n         the arguments used in the downloading process\n\n    Note:\n        The local and obs parameters are optionals:\n        - local (str | None): Local path where the product will be stored. If this\n            parameter is not given, the local path where the file is stored will be set to a temporary one\n        - obs (str | None): Path to S3 storage where the file will be uploaded, after a successfull download from CADIP\n            server. If this parameter is not given, the file will not be uploaded to the s3 storage.\n\n    Returns:\n        None\n\n    Raises:\n        RuntimeError: If there is an issue connecting to the S3 storage during the download.\n    \"\"\"\n\n    # Open a database sessions in this thread, because the session from the root thread may have closed.\n    # Get the product download status\n\n    db_product = argument.db_handler.get(db, name=argument.name)\n    # init eodag object\n    try:\n        logger.debug(\n            \"%s : %s : %s: Thread started !\",\n            os.getpid(),\n            threading.get_ident(),\n            datetime.now(),\n        )\n\n        setup_logging(3, no_progress_bar=True)\n        # tempfile to be used here\n\n        # Update the status to IN_PROGRESS in the database\n        db_product.in_progress(db)\n        local = kwargs[\"default_path\"] if not argument.local else argument.local\n        # notify the main thread that the download will be started\n        # To be discussed: init_provider may fail, but in the same time it takes too much\n        # when properly initialized, and the timeout for download endpoint return is overpassed\n        argument.thread_started.set()\n        provider = init_provider(argument.station)\n        init = datetime.now()\n        filename = Path(local) / argument.name\n        provider.download(argument.product_id, filename)\n        logger.info(\n            \"%s : %s : File: %s downloaded in %s\",\n            os.getpid(),\n            threading.get_ident(),\n            argument.name,\n            datetime.now() - init,\n        )\n    except Exception as exception:  # pylint: disable=broad-exception-caught\n        # Pylint disabled since error is logged here.\n        logger.error(\n            \"%s : %s : %s: Exception caught: %s\",\n            os.getpid(),\n            threading.get_ident(),\n            datetime.now(),\n            exception,\n        )\n\n        # Try n times to update the status to FAILED in the database\n        update_db(db, db_product, EDownloadStatus.FAILED, repr(exception))\n        return\n\n    if argument.obs:\n        try:\n            # NOTE: The environment variables have to be set from outside\n            # otherwise the connection with the s3 endpoint fails\n            # TODO: the secrets should be set through env vars\n            # pylint: disable=pointless-string-statement\n            \"\"\"\n            secrets = {\n                \"s3endpoint\": None,\n                \"accesskey\": None,\n                \"secretkey\": None,\n            }\n            S3StorageHandler.get_secrets_from_file(secrets, \"/home/\" + os.environ[\"USER\"] + \"/.s3cfg\")\n            os.environ[\"S3_ACCESSKEY\"] = secrets[\"accesskey\"]\n            os.environ[\"S3_SECRETKEY\"] = secrets[\"secretkey\"]\n            os.environ[\"S3_ENDPOINT\"] = secrets[\"s3endpoint\"]\n            os.environ[\"S3_REGION\"] = \"sbg\"\n            \"\"\"\n            s3_handler = S3StorageHandler(\n                os.environ[\"S3_ACCESSKEY\"],\n                os.environ[\"S3_SECRETKEY\"],\n                os.environ[\"S3_ENDPOINT\"],\n                os.environ[\"S3_REGION\"],  # \"sbg\",\n            )\n            obs_array = argument.obs.split(\"/\")  # s3://bucket/path/to\n            s3_config = PutFilesToS3Config(\n                [str(filename)],\n                obs_array[2],\n                \"/\".join(obs_array[3:]),\n            )\n            s3_handler.put_files_to_s3(s3_config)\n        except (RuntimeError, KeyError) as e:\n            logger.exception(f\"Could not connect to the s3 storage: {e}\")\n            # Try n times to update the status to FAILED in the database\n            update_db(\n                db,\n                db_product,\n                EDownloadStatus.FAILED,\n                \"Could not connect to the s3 storage\",\n            )\n            return\n        except Exception as e:  # pylint: disable=broad-except\n            logger.exception(f\"General exception: {e}\")\n            return\n        finally:\n            os.remove(filename)\n\n    # Try n times to update the status to DONE in the database\n    update_db(db, db_product, EDownloadStatus.DONE)\n    logger.debug(\"Download finished succesfully for %s\", db_product.name)\n</code></pre>"},{"location":"common/#rs_server_common.utils.utils.extract_eo_product","title":"<code>extract_eo_product(eo_product, mapper)</code>","text":"<p>This function is creating key:value pairs from an EOProduct properties</p> Source code in <code>rs_server_common/utils/utils.py</code> <pre><code>def extract_eo_product(eo_product: EOProduct, mapper: dict) -&gt; dict:\n    \"\"\"This function is creating key:value pairs from an EOProduct properties\"\"\"\n    return {key: value for key, value in eo_product.properties.items() if key in mapper.values()}\n</code></pre>"},{"location":"common/#rs_server_common.utils.utils.is_valid_date_format","title":"<code>is_valid_date_format(date)</code>","text":"<p>Check if a string adheres to the expected date format \"YYYY-MM-DDTHH:MM:SS.sssZ\".</p> <p>Parameters:</p> Name Type Description Default <code>date</code> <code>str</code> <p>The string to be validated for the specified date format.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the input string adheres to the expected date format, otherwise False.</p> Source code in <code>rs_server_common/utils/utils.py</code> <pre><code>def is_valid_date_format(date: str) -&gt; bool:\n    \"\"\"Check if a string adheres to the expected date format \"YYYY-MM-DDTHH:MM:SS.sssZ\".\n\n    Args:\n        date (str): The string to be validated for the specified date format.\n\n    Returns:\n        bool: True if the input string adheres to the expected date format, otherwise False.\n\n    \"\"\"\n    try:\n        datetime.strptime(date, \"%Y-%m-%dT%H:%M:%SZ\")\n        return True\n    except ValueError:\n        return False\n</code></pre>"},{"location":"common/#rs_server_common.utils.utils.odata_to_stac","title":"<code>odata_to_stac(feature_template, odata_dict, odata_stac_mapper)</code>","text":"<p>This function is used to map odata values to a given STAC template</p> Source code in <code>rs_server_common/utils/utils.py</code> <pre><code>def odata_to_stac(feature_template: dict, odata_dict: dict, odata_stac_mapper: dict) -&gt; dict:\n    \"\"\"This function is used to map odata values to a given STAC template\"\"\"\n    if not all(item in feature_template.keys() for item in [\"properties\", \"id\", \"assets\"]):\n        raise ValueError(\"Invalid stac feature template\")\n    for stac_key, eodag_key in odata_stac_mapper.items():\n        if eodag_key in odata_dict:\n            if stac_key in feature_template[\"properties\"]:\n                feature_template[\"properties\"][stac_key] = odata_dict[eodag_key]\n            elif stac_key == \"id\":\n                feature_template[\"id\"] = odata_dict[eodag_key]\n            elif stac_key == \"file:size\":\n                feature_template[\"assets\"][\"file\"][stac_key] = odata_dict[eodag_key]\n    return feature_template\n</code></pre>"},{"location":"common/#rs_server_common.utils.utils.sort_feature_collection","title":"<code>sort_feature_collection(feature_collection, sortby)</code>","text":"<p>This function sorts a STAC feature collection based on a given criteria</p> Source code in <code>rs_server_common/utils/utils.py</code> <pre><code>def sort_feature_collection(feature_collection: dict, sortby: str) -&gt; dict:\n    \"\"\"This function sorts a STAC feature collection based on a given criteria\"\"\"\n    # Force default sorting even if the input is invalid, don't block the return collection because of sorting.\n    if sortby != \"+doNotSort\":\n        order = sortby[0]\n        if order not in [\"+\", \"-\"]:\n            order = \"+\"\n\n        if len(feature_collection[\"features\"]) and \"properties\" in feature_collection[\"features\"][0]:\n            field = sortby[1:]\n            by = \"datetime\" if field not in feature_collection[\"features\"][0][\"properties\"].keys() else field\n            feature_collection[\"features\"] = sorted(\n                feature_collection[\"features\"],\n                key=lambda feature: feature[\"properties\"][by],\n                reverse=order == \"-\",\n            )\n    return feature_collection\n</code></pre>"},{"location":"common/#rs_server_common.utils.utils.update_db","title":"<code>update_db(db, db_product, estatus, status_fail_message=None)</code>","text":"<p>Update the download status of a product in the database.</p> <p>This function attempts to update the download status of a product in the database. It retries the update operation for a maximum of three times, waiting 1 second between attempts.</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>Session</code> <p>The database session.</p> required <code>db_product</code> <code>DownloadStatus</code> <p>The product whose status needs to be updated.</p> required <code>estatus</code> <code>EDownloadStatus</code> <p>The new download status.</p> required <code>status_fail_message</code> <code>Optional[str]</code> <p>An optional message associated with the failure status.</p> <code>None</code> <p>Returns:</p> Type Description <p>None</p> <p>Raises:</p> Type Description <code>OperationalError</code> <p>If the database update operation fails after multiple attempts.</p> Example <p>update_db(db_session, product_instance, EDownloadStatus.DONE)</p> Note <ul> <li>This function is designed to update the download status in the database.</li> <li>It retries the update operation for a maximum of three times.</li> <li>If the update fails, an exception is raised, indicating an issue with the database.</li> </ul> Source code in <code>rs_server_common/utils/utils.py</code> <pre><code>def update_db(\n    db: sqlalchemy.orm.Session,\n    db_product: DownloadStatus,\n    estatus: EDownloadStatus,\n    status_fail_message=None,\n):\n    \"\"\"Update the download status of a product in the database.\n\n    This function attempts to update the download status of a product in the database.\n    It retries the update operation for a maximum of three times, waiting 1 second between attempts.\n\n    Args:\n        db (sqlalchemy.orm.Session): The database session.\n        db_product (DownloadStatus): The product whose status needs to be updated.\n        estatus (EDownloadStatus): The new download status.\n        status_fail_message (Optional[str]): An optional message associated with the failure status.\n\n    Returns:\n        None\n\n    Raises:\n        sqlalchemy.exc.OperationalError: If the database update operation fails after multiple attempts.\n\n    Example:\n        &gt;&gt;&gt; update_db(db_session, product_instance, EDownloadStatus.DONE)\n\n    Note:\n        - This function is designed to update the download status in the database.\n        - It retries the update operation for a maximum of three times.\n        - If the update fails, an exception is raised, indicating an issue with the database.\n\n    \"\"\"\n    # Try n times to update the status.\n    # Don't do it for NOT_STARTED and IN_PROGRESS (call directly db_product.not_started\n    # or db_product.in_progress) because it will anyway be overwritten later by DONE or FAILED.\n\n    # Init last exception to empty value.\n    last_exception: Exception = Exception()\n\n    for _ in range(3):\n        try:\n            if estatus == EDownloadStatus.FAILED:\n                db_product.failed(db, status_fail_message)\n            elif estatus == EDownloadStatus.DONE:\n                db_product.done(db)\n\n            # The database update worked, exit function\n            return\n\n        # The database update failed, wait n seconds and retry\n        except sqlalchemy.exc.OperationalError as exception:\n            logger.error(f\"Error updating status in database:\\n{exception}\")\n            last_exception = exception\n            time.sleep(1)\n\n    # If all attemps failed, raise the last Exception\n    raise last_exception\n</code></pre>"},{"location":"common/#rs_server_common.utils.utils.validate_inputs_format","title":"<code>validate_inputs_format(interval)</code>","text":"<p>Validate the format of the input time interval.</p> <p>This function checks whether the input interval has a valid format (start_date/stop_date) and whether the start and stop dates are in a valid ISO 8601 format.</p> <p>Parameters:</p> Name Type Description Default <code>-</code> <code>interval (str</code> <p>The time interval to be validated, with the following format: \"2024-01-01T00:00:00Z/2024-01-02T23:59:59Z\"</p> required <p>Returns:</p> Type Description <code>Tuple[datetime, datetime]</code> <p>Tuple[datetime, datetime]: A tuple containing: - start_date (datetime): The start date of the interval. - stop_date (datetime): The stop date of the interval.</p> Note <ul> <li>The input interval should be in the format \"start_date/stop_date\" (e.g., \"2022-01-01T00:00:00Z/2022-01-02T00:00:00Z\").</li> <li>This function checks for missing start/stop and validates the ISO 8601 format of start and stop dates.</li> <li>If there is an error, err_code and err_text provide information about the issue.</li> </ul> Source code in <code>rs_server_common/utils/utils.py</code> <pre><code>def validate_inputs_format(interval: str) -&gt; Tuple[datetime, datetime]:\n    \"\"\"\n    Validate the format of the input time interval.\n\n    This function checks whether the input interval has a valid format (start_date/stop_date) and\n    whether the start and stop dates are in a valid ISO 8601 format.\n\n    Args:\n        - interval (str): The time interval to be validated, with the following format:\n            \"2024-01-01T00:00:00Z/2024-01-02T23:59:59Z\"\n\n    Returns:\n        Tuple[datetime, datetime]:\n            A tuple containing:\n            - start_date (datetime): The start date of the interval.\n            - stop_date (datetime): The stop date of the interval.\n\n    Note:\n        - The input interval should be in the format \"start_date/stop_date\"\n        (e.g., \"2022-01-01T00:00:00Z/2022-01-02T00:00:00Z\").\n        - This function checks for missing start/stop and validates the ISO 8601 format of start and stop dates.\n        - If there is an error, err_code and err_text provide information about the issue.\n    \"\"\"\n    try:\n        start_date, stop_date = interval.split(\"/\")\n    except ValueError as exc:\n        logger.error(\"Missing start or stop in endpoint call!\")\n        raise HTTPException(status_code=status.HTTP_422_UNPROCESSABLE_ENTITY, detail=\"Missing start/stop\") from exc\n    if (not is_valid_date_format(start_date)) or (not is_valid_date_format(stop_date)):\n        logger.error(\"Invalid start/stop in endpoint call!\")\n        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=\"Missing start/stop\")\n\n    return datetime.fromisoformat(start_date), datetime.fromisoformat(stop_date)\n</code></pre>"},{"location":"common/#rs_server_common.utils.utils.write_search_products_to_db","title":"<code>write_search_products_to_db(db_handler_class, products)</code>","text":"<p>Process a list of products by adding them to the database if not already present.</p> <p>This function iterates over a list of products. For each product, it checks whether the product is already registered in the database. If the product is not in the database, it is added with its relevant details. The function collects a list of product IDs and names for further processing.</p> <p>Parameters:</p> Name Type Description Default <code>-</code> <code>products (List[Product]</code> <p>A list of product objects to be processed.</p> required <code>-</code> <code>db_handler_class</code> <p>The database handler class used for database operations.</p> required <p>Returns:</p> Type Description <code>None</code> <p>List[Tuple]: A list of tuples, each containing the 'id' and 'Name' properties of a product.</p> <p>Raises:</p> Type Description <code>-OperationalError</code> <p>If there's an issue connecting to the database.</p> Note <ul> <li>The function assumes that 'products' is a list of objects with a 'properties' attribute,   which is a dictionary containing keys 'id', 'Name', and 'startTimeFromAscendingNode'.</li> <li>'get_db' is a context manager that provides a database session.</li> <li>'EDownloadStatus' is an enumeration representing download status.</li> </ul> Source code in <code>rs_server_common/utils/utils.py</code> <pre><code>def write_search_products_to_db(db_handler_class: DownloadStatus, products: EOProduct) -&gt; None:\n    \"\"\"\n    Process a list of products by adding them to the database if not already present.\n\n    This function iterates over a list of products. For each product, it checks whether the product\n    is already registered in the database. If the product is not in the database, it is added with\n    its relevant details. The function collects a list of product IDs and names for further processing.\n\n    Args:\n        - products (List[Product]): A list of product objects to be processed.\n        - db_handler_class: The database handler class used for database operations.\n\n    Returns:\n        List[Tuple]: A list of tuples, each containing the 'id' and 'Name' properties of a product.\n\n    Raises:\n        - sqlalchemy.exc.OperationalError: If there's an issue connecting to the database.\n\n    Note:\n        - The function assumes that 'products' is a list of objects with a 'properties' attribute,\n          which is a dictionary containing keys 'id', 'Name', and 'startTimeFromAscendingNode'.\n        - 'get_db' is a context manager that provides a database session.\n        - 'EDownloadStatus' is an enumeration representing download status.\n    \"\"\"\n    with contextmanager(get_db)() as db:\n        try:\n            for product in products:\n                if db_handler_class.get_if_exists(db, product.properties[\"Name\"]) is not None:\n                    logger.info(\n                        \"Product %s is already registered in database, skipping\",\n                        product.properties[\"Name\"],\n                    )\n                    continue\n\n                db_handler_class.create(\n                    db,\n                    product_id=product.properties[\"id\"],\n                    name=product.properties[\"Name\"],\n                    available_at_station=datetime.fromisoformat(product.properties[\"startTimeFromAscendingNode\"]),\n                    status=EDownloadStatus.NOT_STARTED,\n                )\n\n        except sqlalchemy.exc.OperationalError:\n            logger.error(\"Failed to connect with DB during listing procedure\")\n            raise\n</code></pre>"},{"location":"common/#rs_server_common.data_retrieval.eodag_provider.EodagProvider","title":"<code>EodagProvider</code>","text":"<p>               Bases: <code>Provider</code></p> <p>An EODAG provider.</p> <p>It uses EODAG to provide data from external sources.</p> Source code in <code>rs_server_common/data_retrieval/eodag_provider.py</code> <pre><code>class EodagProvider(Provider):\n    \"\"\"An EODAG provider.\n\n    It uses EODAG to provide data from external sources.\n    \"\"\"\n\n    lock = Lock()  # static Lock instance\n\n    def __init__(self, config_file: Path, provider: str):\n        \"\"\"Create a EODAG provider.\n\n        Args:\n            config_file: the path to the eodag configuration file\n            provider: the name of the eodag provider\n        \"\"\"\n        self.eodag_cfg_dir = tempfile.TemporaryDirectory()  # pylint: disable=consider-using-with\n        self.provider: str = provider\n        self.config_file = config_file\n        self.client: EODataAccessGateway = self.init_eodag_client(config_file)\n        self.client.set_preferred_provider(self.provider)\n\n    def __del__(self):\n        \"\"\"Destructor\"\"\"\n        try:\n            shutil.rmtree(self.eodag_cfg_dir.name)  # remove the unique /tmp dir\n        except FileNotFoundError:\n            pass\n\n    def init_eodag_client(self, config_file: Path) -&gt; EODataAccessGateway:\n        \"\"\"Initialize the eodag client.\n\n        The EODAG client is initialized for the given provider.\n\n        Args:\n            config_file: the path to the eodag configuration file\n\n        Returns:\n             the initialized eodag client\n        \"\"\"\n        try:\n            # Use thread-lock\n            with EodagProvider.lock:\n                os.environ[\"EODAG_CFG_DIR\"] = self.eodag_cfg_dir.name\n                return EODataAccessGateway(config_file.as_posix())\n        except Exception as e:\n            raise CreateProviderFailed(f\"Can't initialize {self.provider} provider\") from e\n\n    def _specific_search(self, between: TimeRange, **kwargs) -&gt; Union[SearchResult, List]:\n        \"\"\"\n        Conducts a search for products within a specified time range.\n\n        This private method interfaces with the client's search functionality,\n        retrieving products that fall within the given time range. The 'between'\n        parameter is expected to be a TimeRange object, encompassing start and end\n        timestamps. The method returns a dictionary of products keyed by their\n        respective identifiers.\n\n        Args:\n            between (TimeRange): An object representing the start and end timestamps\n                                for the search range.\n\n        Returns:\n            SearchResult: A dictionary where keys are product identifiers and\n                            values are EOProduct instances.\n\n        Note:\n            The time format of the 'between' parameter should be verified or formatted\n            appropriately before invoking this method. The method also assumes that the\n            client's search function is correctly set up to handle the provided time\n            range format.\n\n        Raises:\n            Exception: If the search encounters an error or fails, an exception is raised.\n        \"\"\"\n        mapped_search_args = {}\n        if kwargs.pop(\"sessions_search\", False):\n            if session_id := kwargs.pop(\"id\", None):\n                if isinstance(session_id, list):\n                    mapped_search_args.update({\"SessionIds\": \", \".join(session_id)})\n                elif isinstance(session_id, str):\n                    mapped_search_args.update({\"SessionId\": session_id})\n            if platform := kwargs.pop(\"platform\", None):\n                if isinstance(platform, list):\n                    mapped_search_args.update({\"platforms\": \", \".join(platform)})\n                elif isinstance(platform, str):\n                    mapped_search_args.update({\"platform\": platform})\n            if between:\n                mapped_search_args.update(\n                    {\n                        \"startTimeFromAscendingNode\": str(between.start),\n                        \"completionTimeFromAscendingNode\": str(between.end),\n                    },\n                )\n            try:\n                products, _ = self.client.search(\n                    **mapped_search_args,  # type: ignore\n                    provider=self.provider,\n                    raise_errors=True,\n                    **kwargs,\n                )\n            except RequestError:\n                return []\n        else:\n            try:\n                products, _ = self.client.search(\n                    start=str(between.start),\n                    end=str(between.end),\n                    provider=self.provider,\n                    raise_errors=True,\n                    **kwargs,\n                )\n            except RequestError:\n                return []\n        return products\n\n    def download(self, product_id: str, to_file: Path) -&gt; None:\n        \"\"\"Download the expected product at the given local location.\n\n        EODAG needs an EOProduct to download.\n        We build an EOProduct from the id and download location\n        to be able to call EODAG for download.\n\n\n        Args:\n            product_id: the id of the product to download\n            to_file: the path where the product has to be download\n\n        Returns:\n            None\n\n        \"\"\"\n        product = self.create_eodag_product(product_id, to_file.name)\n        # download_plugin = self.client._plugins_manager.get_download_plugin(product)\n        # authent_plugin = self.client._plugins_manager.get_auth_plugin(product.provider)\n        # product.register_downloader(download_plugin, authent_plugin)\n        self.client.download(product, outputs_prefix=to_file.parent)\n\n    def create_eodag_product(self, product_id, filename):\n        \"\"\"Initialize an EO product with minimal properties.\n\n        The title is used by EODAG as the name of the downloaded file.\n        The download link is used by EODAG as http request url for download.\n        The geometry is mandatory in an EO Product so we add the all earth as geometry.\n\n        Args:\n            product_id: the id of EO Product\n            filename: the name of the downloaded file\n\n        Returns:\n            the initialized EO Product\n\n        \"\"\"\n        try:\n            with open(self.config_file, \"r\", encoding=\"utf-8\") as f:\n                base_uri = yaml.safe_load(f)[self.provider.lower()][\"download\"][\"base_uri\"]\n            return EOProduct(\n                self.provider,\n                {\n                    \"id\": product_id,\n                    \"title\": filename,\n                    \"geometry\": \"POLYGON((180 -90, 180 90, -180 90, -180 -90, 180 -90))\",\n                    # TODO build from configuration (but how ?)\n                    \"downloadLink\": f\"{base_uri}({product_id})/$value\",\n                },\n            )\n        except Exception as e:\n            raise CreateProviderFailed(f\"Can't initialize {self.provider} download provider\") from e\n</code></pre>"},{"location":"common/#rs_server_common.data_retrieval.eodag_provider.EodagProvider.__del__","title":"<code>__del__()</code>","text":"<p>Destructor</p> Source code in <code>rs_server_common/data_retrieval/eodag_provider.py</code> <pre><code>def __del__(self):\n    \"\"\"Destructor\"\"\"\n    try:\n        shutil.rmtree(self.eodag_cfg_dir.name)  # remove the unique /tmp dir\n    except FileNotFoundError:\n        pass\n</code></pre>"},{"location":"common/#rs_server_common.data_retrieval.eodag_provider.EodagProvider.__init__","title":"<code>__init__(config_file, provider)</code>","text":"<p>Create a EODAG provider.</p> <p>Parameters:</p> Name Type Description Default <code>config_file</code> <code>Path</code> <p>the path to the eodag configuration file</p> required <code>provider</code> <code>str</code> <p>the name of the eodag provider</p> required Source code in <code>rs_server_common/data_retrieval/eodag_provider.py</code> <pre><code>def __init__(self, config_file: Path, provider: str):\n    \"\"\"Create a EODAG provider.\n\n    Args:\n        config_file: the path to the eodag configuration file\n        provider: the name of the eodag provider\n    \"\"\"\n    self.eodag_cfg_dir = tempfile.TemporaryDirectory()  # pylint: disable=consider-using-with\n    self.provider: str = provider\n    self.config_file = config_file\n    self.client: EODataAccessGateway = self.init_eodag_client(config_file)\n    self.client.set_preferred_provider(self.provider)\n</code></pre>"},{"location":"common/#rs_server_common.data_retrieval.eodag_provider.EodagProvider.create_eodag_product","title":"<code>create_eodag_product(product_id, filename)</code>","text":"<p>Initialize an EO product with minimal properties.</p> <p>The title is used by EODAG as the name of the downloaded file. The download link is used by EODAG as http request url for download. The geometry is mandatory in an EO Product so we add the all earth as geometry.</p> <p>Parameters:</p> Name Type Description Default <code>product_id</code> <p>the id of EO Product</p> required <code>filename</code> <p>the name of the downloaded file</p> required <p>Returns:</p> Type Description <p>the initialized EO Product</p> Source code in <code>rs_server_common/data_retrieval/eodag_provider.py</code> <pre><code>def create_eodag_product(self, product_id, filename):\n    \"\"\"Initialize an EO product with minimal properties.\n\n    The title is used by EODAG as the name of the downloaded file.\n    The download link is used by EODAG as http request url for download.\n    The geometry is mandatory in an EO Product so we add the all earth as geometry.\n\n    Args:\n        product_id: the id of EO Product\n        filename: the name of the downloaded file\n\n    Returns:\n        the initialized EO Product\n\n    \"\"\"\n    try:\n        with open(self.config_file, \"r\", encoding=\"utf-8\") as f:\n            base_uri = yaml.safe_load(f)[self.provider.lower()][\"download\"][\"base_uri\"]\n        return EOProduct(\n            self.provider,\n            {\n                \"id\": product_id,\n                \"title\": filename,\n                \"geometry\": \"POLYGON((180 -90, 180 90, -180 90, -180 -90, 180 -90))\",\n                # TODO build from configuration (but how ?)\n                \"downloadLink\": f\"{base_uri}({product_id})/$value\",\n            },\n        )\n    except Exception as e:\n        raise CreateProviderFailed(f\"Can't initialize {self.provider} download provider\") from e\n</code></pre>"},{"location":"common/#rs_server_common.data_retrieval.eodag_provider.EodagProvider.download","title":"<code>download(product_id, to_file)</code>","text":"<p>Download the expected product at the given local location.</p> <p>EODAG needs an EOProduct to download. We build an EOProduct from the id and download location to be able to call EODAG for download.</p> <p>Parameters:</p> Name Type Description Default <code>product_id</code> <code>str</code> <p>the id of the product to download</p> required <code>to_file</code> <code>Path</code> <p>the path where the product has to be download</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>rs_server_common/data_retrieval/eodag_provider.py</code> <pre><code>def download(self, product_id: str, to_file: Path) -&gt; None:\n    \"\"\"Download the expected product at the given local location.\n\n    EODAG needs an EOProduct to download.\n    We build an EOProduct from the id and download location\n    to be able to call EODAG for download.\n\n\n    Args:\n        product_id: the id of the product to download\n        to_file: the path where the product has to be download\n\n    Returns:\n        None\n\n    \"\"\"\n    product = self.create_eodag_product(product_id, to_file.name)\n    # download_plugin = self.client._plugins_manager.get_download_plugin(product)\n    # authent_plugin = self.client._plugins_manager.get_auth_plugin(product.provider)\n    # product.register_downloader(download_plugin, authent_plugin)\n    self.client.download(product, outputs_prefix=to_file.parent)\n</code></pre>"},{"location":"common/#rs_server_common.data_retrieval.eodag_provider.EodagProvider.init_eodag_client","title":"<code>init_eodag_client(config_file)</code>","text":"<p>Initialize the eodag client.</p> <p>The EODAG client is initialized for the given provider.</p> <p>Parameters:</p> Name Type Description Default <code>config_file</code> <code>Path</code> <p>the path to the eodag configuration file</p> required <p>Returns:</p> Type Description <code>EODataAccessGateway</code> <p>the initialized eodag client</p> Source code in <code>rs_server_common/data_retrieval/eodag_provider.py</code> <pre><code>def init_eodag_client(self, config_file: Path) -&gt; EODataAccessGateway:\n    \"\"\"Initialize the eodag client.\n\n    The EODAG client is initialized for the given provider.\n\n    Args:\n        config_file: the path to the eodag configuration file\n\n    Returns:\n         the initialized eodag client\n    \"\"\"\n    try:\n        # Use thread-lock\n        with EodagProvider.lock:\n            os.environ[\"EODAG_CFG_DIR\"] = self.eodag_cfg_dir.name\n            return EODataAccessGateway(config_file.as_posix())\n    except Exception as e:\n        raise CreateProviderFailed(f\"Can't initialize {self.provider} provider\") from e\n</code></pre>"},{"location":"common/#rs_server_common.data_retrieval.provider.CreateProviderFailed","title":"<code>CreateProviderFailed</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when an error occurred during the init of a provider.</p> Source code in <code>rs_server_common/data_retrieval/provider.py</code> <pre><code>class CreateProviderFailed(Exception):\n    \"\"\"Exception raised when an error occurred during the init of a provider.\"\"\"\n</code></pre>"},{"location":"common/#rs_server_common.data_retrieval.provider.DownloadProductFailed","title":"<code>DownloadProductFailed</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when an error occurred during the download.</p> Source code in <code>rs_server_common/data_retrieval/provider.py</code> <pre><code>class DownloadProductFailed(Exception):\n    \"\"\"Exception raised when an error occurred during the download.\"\"\"\n</code></pre>"},{"location":"common/#rs_server_common.data_retrieval.provider.Product","title":"<code>Product</code>  <code>dataclass</code>","text":"<p>A product.</p> <p>A product has an external identifier and a dictionary of metadata.</p> Source code in <code>rs_server_common/data_retrieval/provider.py</code> <pre><code>@dataclass\nclass Product:\n    \"\"\"A product.\n\n    A product has an external identifier and a dictionary of metadata.\n    \"\"\"\n\n    id_: str\n    metadata: dict[str, str]\n</code></pre>"},{"location":"common/#rs_server_common.data_retrieval.provider.Provider","title":"<code>Provider</code>","text":"<p>               Bases: <code>ABC</code></p> <p>A product provider.</p> <p>A provider gives a common interface to search for files from an external data source and download them locally.</p> Source code in <code>rs_server_common/data_retrieval/provider.py</code> <pre><code>class Provider(ABC):\n    \"\"\"A product provider.\n\n    A provider gives a common interface to search for files from an external data source\n    and download them locally.\n    \"\"\"\n\n    def search(self, between: TimeRange, **kwargs) -&gt; Any:\n        \"\"\"Search for products with the given time range.\n\n        The search result is a dictionary of products found indexed by id.\n\n        Args:\n            between: the search period\n\n        Returns:\n            The files found indexed by file id. Specific to each provider.\n\n        \"\"\"\n        if between:\n            if between.duration() == timedelta(0):\n                return []\n            if between.duration() &lt; timedelta(0):\n                raise SearchProductFailed(f\"Search timerange is inverted : ({between.start} -&gt; {between.end})\")\n        return self._specific_search(between, **kwargs)\n\n    @abstractmethod\n    def _specific_search(self, between: TimeRange) -&gt; Any:\n        \"\"\"Search for products with the given time range.\n\n        Specific search for products after common verification.\n\n        Args:\n            between: the search period\n\n        Returns:\n            the files found indexed by file id.\n\n        \"\"\"\n\n    @abstractmethod\n    def download(self, product_id: str, to_file: Path) -&gt; None:\n        \"\"\"Download the given product to the given local path.\n\n        Args:\n            product_id: id of the product to download\n            to_file: path where the file should be downloaded\n\n        Returns:\n            None\n\n        \"\"\"\n</code></pre>"},{"location":"common/#rs_server_common.data_retrieval.provider.Provider.download","title":"<code>download(product_id, to_file)</code>  <code>abstractmethod</code>","text":"<p>Download the given product to the given local path.</p> <p>Parameters:</p> Name Type Description Default <code>product_id</code> <code>str</code> <p>id of the product to download</p> required <code>to_file</code> <code>Path</code> <p>path where the file should be downloaded</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>rs_server_common/data_retrieval/provider.py</code> <pre><code>@abstractmethod\ndef download(self, product_id: str, to_file: Path) -&gt; None:\n    \"\"\"Download the given product to the given local path.\n\n    Args:\n        product_id: id of the product to download\n        to_file: path where the file should be downloaded\n\n    Returns:\n        None\n\n    \"\"\"\n</code></pre>"},{"location":"common/#rs_server_common.data_retrieval.provider.Provider.search","title":"<code>search(between, **kwargs)</code>","text":"<p>Search for products with the given time range.</p> <p>The search result is a dictionary of products found indexed by id.</p> <p>Parameters:</p> Name Type Description Default <code>between</code> <code>TimeRange</code> <p>the search period</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The files found indexed by file id. Specific to each provider.</p> Source code in <code>rs_server_common/data_retrieval/provider.py</code> <pre><code>def search(self, between: TimeRange, **kwargs) -&gt; Any:\n    \"\"\"Search for products with the given time range.\n\n    The search result is a dictionary of products found indexed by id.\n\n    Args:\n        between: the search period\n\n    Returns:\n        The files found indexed by file id. Specific to each provider.\n\n    \"\"\"\n    if between:\n        if between.duration() == timedelta(0):\n            return []\n        if between.duration() &lt; timedelta(0):\n            raise SearchProductFailed(f\"Search timerange is inverted : ({between.start} -&gt; {between.end})\")\n    return self._specific_search(between, **kwargs)\n</code></pre>"},{"location":"common/#rs_server_common.data_retrieval.provider.SearchProductFailed","title":"<code>SearchProductFailed</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when an error occurred during the search.</p> Source code in <code>rs_server_common/data_retrieval/provider.py</code> <pre><code>class SearchProductFailed(Exception):\n    \"\"\"Exception raised when an error occurred during the search.\"\"\"\n</code></pre>"},{"location":"common/#rs_server_common.data_retrieval.provider.TimeRange","title":"<code>TimeRange</code>  <code>dataclass</code>","text":"<p>A time range.</p> Source code in <code>rs_server_common/data_retrieval/provider.py</code> <pre><code>@dataclass\nclass TimeRange:\n    \"\"\"A time range.\"\"\"\n\n    start: datetime\n    end: datetime\n\n    def duration(self) -&gt; timedelta:\n        \"\"\"Duration of the timerange.\n\n        Returns: duration of the timerange\n        \"\"\"\n        return self.end - self.start\n\n    def __bool__(self) -&gt; bool:\n        return self.start is not None and self.end is not None\n</code></pre>"},{"location":"common/#rs_server_common.data_retrieval.provider.TimeRange.duration","title":"<code>duration()</code>","text":"<p>Duration of the timerange.</p> <p>Returns: duration of the timerange</p> Source code in <code>rs_server_common/data_retrieval/provider.py</code> <pre><code>def duration(self) -&gt; timedelta:\n    \"\"\"Duration of the timerange.\n\n    Returns: duration of the timerange\n    \"\"\"\n    return self.end - self.start\n</code></pre>"},{"location":"common/#rs_server_common.s3_storage_handler.s3_storage_handler.GetKeysFromS3Config","title":"<code>GetKeysFromS3Config</code>  <code>dataclass</code>","text":"<p>S3 configuration for download</p> <p>Attributes:</p> Name Type Description <code>s3_files</code> <code>list</code> <p>A list with the  S3 object keys to be downloaded.</p> <code>bucket</code> <code>str</code> <p>The S3 bucket name.</p> <code>local_prefix</code> <code>str</code> <p>The local prefix where files will be downloaded.</p> <code>overwrite</code> <code>bool</code> <p>Flag indicating whether to overwrite existing files. Default is False.</p> <code>max_retries</code> <code>int</code> <p>The maximum number of download retries. Default is DWN_S3FILE_RETRIES.</p> Source code in <code>rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>@dataclass\nclass GetKeysFromS3Config:\n    \"\"\"S3 configuration for download\n\n    Attributes:\n        s3_files (list): A list with the  S3 object keys to be downloaded.\n        bucket (str): The S3 bucket name.\n        local_prefix (str): The local prefix where files will be downloaded.\n        overwrite (bool, optional): Flag indicating whether to overwrite existing files. Default is False.\n        max_retries (int, optional): The maximum number of download retries. Default is DWN_S3FILE_RETRIES.\n\n    \"\"\"\n\n    s3_files: list\n    bucket: str\n    local_prefix: str\n    overwrite: bool = False\n    max_retries: int = DWN_S3FILE_RETRIES\n</code></pre>"},{"location":"common/#rs_server_common.s3_storage_handler.s3_storage_handler.PutFilesToS3Config","title":"<code>PutFilesToS3Config</code>  <code>dataclass</code>","text":"<p>Configuration for uploading files to S3.</p> <p>Attributes:</p> Name Type Description <code>files</code> <code>List</code> <p>A list with the local file paths to be uploaded.</p> <code>bucket</code> <code>str</code> <p>The S3 bucket name.</p> <code>s3_path</code> <code>str</code> <p>The S3 path where files will be uploaded.</p> <code>max_retries</code> <code>int</code> <p>The maximum number of upload retries. Default is UP_S3FILE_RETRIES.</p> Source code in <code>rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>@dataclass\nclass PutFilesToS3Config:\n    \"\"\"Configuration for uploading files to S3.\n\n    Attributes:\n        files (List): A list with the local file paths to be uploaded.\n        bucket (str): The S3 bucket name.\n        s3_path (str): The S3 path where files will be uploaded.\n        max_retries (int, optional): The maximum number of upload retries. Default is UP_S3FILE_RETRIES.\n\n    \"\"\"\n\n    files: list\n    bucket: str\n    s3_path: str\n    max_retries: int = UP_S3FILE_RETRIES\n</code></pre>"},{"location":"common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler","title":"<code>S3StorageHandler</code>","text":"<p>Interacts with an S3 storage</p> <p>S3StorageHandler for interacting with an S3 storage service.</p> <p>Attributes:</p> Name Type Description <code>access_key_id</code> <code>str</code> <p>The access key ID for S3 authentication.</p> <code>secret_access_key</code> <code>str</code> <p>The secret access key for S3 authentication.</p> <code>endpoint_url</code> <code>str</code> <p>The endpoint URL for the S3 service.</p> <code>region_name</code> <code>str</code> <p>The region name.</p> <code>s3_client</code> <code>client</code> <p>The s3 client to interact with the s3 storage</p> Source code in <code>rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>class S3StorageHandler:\n    \"\"\"Interacts with an S3 storage\n\n    S3StorageHandler for interacting with an S3 storage service.\n\n    Attributes:\n        access_key_id (str): The access key ID for S3 authentication.\n        secret_access_key (str): The secret access key for S3 authentication.\n        endpoint_url (str): The endpoint URL for the S3 service.\n        region_name (str): The region name.\n        s3_client (boto3.client): The s3 client to interact with the s3 storage\n    \"\"\"\n\n    def __init__(self, access_key_id, secret_access_key, endpoint_url, region_name):\n        \"\"\"Initialize the S3StorageHandler instance.\n\n        Args:\n            access_key_id (str): The access key ID for S3 authentication.\n            secret_access_key (str): The secret access key for S3 authentication.\n            endpoint_url (str): The endpoint URL for the S3 service.\n            region_name (str): The region name.\n\n        Raises:\n            RuntimeError: If the connection to the S3 storage cannot be established.\n        \"\"\"\n        self.logger = Logging.default(__name__)\n\n        self.access_key_id = access_key_id\n        self.secret_access_key = secret_access_key\n        self.endpoint_url = endpoint_url\n        self.region_name = region_name\n        self.s3_client: boto3.client = None\n        self.connect_s3()\n        self.logger.debug(\"S3StorageHandler created !\")\n\n    def __get_s3_client(self, access_key_id, secret_access_key, endpoint_url, region_name):\n        \"\"\"Retrieve or create an S3 client instance.\n\n        Args:\n            access_key_id (str): The access key ID for S3 authentication.\n            secret_access_key (str): The secret access key for S3 authentication.\n            endpoint_url (str): The endpoint URL for the S3 service.\n            region_name (str): The region name.\n\n        Returns:\n            boto3.client: An S3 client instance.\n        \"\"\"\n\n        client_config = botocore.config.Config(\n            max_pool_connections=100,\n            # timeout for connection\n            connect_timeout=5,\n            # attempts in trying connection\n            # note:  the default behaviour of boto3 is retrying\n            # connections multiple times and exponentially backing off in between\n            retries={\"total_max_attempts\": 5},\n        )\n        try:\n            return boto3.client(\n                \"s3\",\n                aws_access_key_id=access_key_id,\n                aws_secret_access_key=secret_access_key,\n                endpoint_url=endpoint_url,\n                region_name=region_name,\n                config=client_config,\n            )\n\n        except Exception as e:\n            self.logger.exception(f\"Client error exception: {e}\")\n            raise RuntimeError(\"Client error exception \") from e\n\n    def connect_s3(self):\n        \"\"\"Establish a connection to the S3 service.\n\n        If the S3 client is not already instantiated, this method calls the private __get_s3_client\n        method to create an S3 client instance using the provided credentials and configuration (see __init__).\n        \"\"\"\n        if self.s3_client is None:\n            self.s3_client = self.__get_s3_client(\n                self.access_key_id,\n                self.secret_access_key,\n                self.endpoint_url,\n                self.region_name,\n            )\n\n    def disconnect_s3(self):\n        \"\"\"Close the connection to the S3 service.\"\"\"\n        if self.s3_client is None:\n            return\n        self.s3_client.close()\n        self.s3_client = None\n\n    def delete_file_from_s3(self, bucket, s3_obj):\n        \"\"\"Delete a file from S3.\n\n        Args:\n            bucket (str): The S3 bucket name.\n            s3_obj (str): The S3 object key.\n\n        Raises:\n            RuntimeError: If an error occurs during the bucket access check.\n        \"\"\"\n        if self.s3_client is None or bucket is None or s3_obj is None:\n            raise RuntimeError(\"Input error for deleting the file\")\n        try:\n            self.logger.info(\"Delete key s3://%s/%s\", bucket, s3_obj)\n            self.s3_client.delete_object(Bucket=bucket, Key=s3_obj)\n        except botocore.client.ClientError as e:\n            self.logger.exception(f\"Failed to delete key s3://{bucket}/{s3_obj}: {e}\")\n            raise RuntimeError(f\"Failed to delete key s3://{bucket}/{s3_obj}\") from e\n        except Exception as e:\n            self.logger.exception(f\"Failed to delete key s3://{bucket}/{s3_obj}: {e}\")\n            raise RuntimeError(f\"Failed to delete key s3://{bucket}/{s3_obj}\") from e\n\n    # helper functions\n\n    @staticmethod\n    def get_secrets_from_file(secrets, secret_file):\n        \"\"\"Read secrets from a specified file.\n\n        It reads the secrets from .s3cfg or aws credentials files\n        This function should not be used in production\n\n        Args:\n            secrets (dict): Dictionary to store retrieved secrets.\n            secret_file (str): Path to the file containing secrets.\n            logger (Logger, optional): Logger instance for error logging.\n        \"\"\"\n        dict_filled = 0\n        with open(secret_file, \"r\", encoding=\"utf-8\") as aws_credentials_file:\n            lines = aws_credentials_file.readlines()\n            for line in lines:\n                if not secrets[\"s3endpoint\"] and \"host_bucket\" in line:\n                    dict_filled += 1\n                    secrets[\"s3endpoint\"] = line.strip().split(\"=\")[1].strip()\n                elif not secrets[\"accesskey\"] and \"access_key\" in line:\n                    dict_filled += 1\n                    secrets[\"accesskey\"] = line.strip().split(\"=\")[1].strip()\n                elif not secrets[\"secretkey\"] and \"secret_\" in line and \"_key\" in line:\n                    dict_filled += 1\n                    secrets[\"secretkey\"] = line.strip().split(\"=\")[1].strip()\n                if dict_filled == 3:\n                    break\n\n    @staticmethod\n    def get_basename(input_path):\n        \"\"\"Get the filename from a full path.\n\n        Args:\n            int_path (str): The full path.\n\n        Returns:\n            str: The filename.\n        \"\"\"\n        path, filename = ntpath.split(input_path)\n        return filename or ntpath.basename(path)\n\n    @staticmethod\n    def s3_path_parser(s3_url):\n        \"\"\"\n        Parses S3 URL to extract bucket, prefix, and file.\n\n        Args:\n            s3_url (str): The S3 URL.\n\n        Returns:\n            tuple: Tuple containing bucket, prefix, and file.\n        \"\"\"\n        s3_data = s3_url.replace(\"s3://\", \"\").split(\"/\")\n        bucket = \"\"\n        start_idx = 0\n        if s3_url.startswith(\"s3://\"):\n            bucket = s3_data[0]\n\n            start_idx = 1\n        prefix = \"\"\n        if start_idx &lt; len(s3_data):\n            prefix = \"/\".join(s3_data[start_idx:-1])\n        s3_file = s3_data[-1]\n        return bucket, prefix, s3_file\n\n    def files_to_be_downloaded(self, bucket, paths):\n        \"\"\"Create a list with the S3 keys to be downloaded.\n\n        The list will have the s3 keys to be downloaded from the bucket.\n        It contains pairs (local_prefix_where_the_file_will_be_downloaded, full_s3_key_path)\n        If a s3 key doesn't exist, the pair will be (None, requested_s3_key_path)\n\n        Args:\n            bucket (str): The S3 bucket name.\n            paths (list): List of S3 object keys.\n\n        Returns:\n            list: List of tuples (local_prefix, full_s3_key_path).\n        \"\"\"\n        # declaration of the list\n        list_with_files: List[Any] = []\n        # for each key, identify it as a file or a folder\n        # in the case of a folder, the files will be recursively gathered\n        for key in paths:\n            path = key.strip().lstrip(\"/\")\n            s3_files = self.list_s3_files_obj(bucket, path)\n            if len(s3_files) == 0:\n                self.logger.warning(\"No key %s found.\", path)\n                list_with_files.append((None, path))\n                continue\n            self.logger.debug(\"total: %s | s3_files = %s\", len(s3_files), s3_files)\n            basename_part = self.get_basename(path)\n\n            # check if it's a file or a dir\n            if len(s3_files) == 1 and path == s3_files[0]:\n                # the current key is a file, append it to the list\n                list_with_files.append((\"\", s3_files[0]))\n                self.logger.debug(\"Append files: list_with_files = %s\", list_with_files)\n            else:\n                # the current key is a folder, append all its files (reursively gathered) to the list\n                for s3_file in s3_files:\n                    split = s3_file.split(\"/\")\n                    split_idx = split.index(basename_part)\n                    list_with_files.append((os.path.join(*split[split_idx:-1]), s3_file.strip(\"/\")))\n\n        return list_with_files\n\n    def files_to_be_uploaded(self, paths):\n        \"\"\"Creates a list with the local files to be uploaded.\n\n        The list contains pairs (s3_path, absolute_local_file_path)\n        If the local file doesn't exist, the pair will be (None, requested_file_to_upload)\n\n        Args:\n            paths (list): List of local file paths.\n\n        Returns:\n            list: List of tuples (s3_path, absolute_local_file_path).\n        \"\"\"\n\n        list_with_files = []\n        for local in paths:\n            path = local.strip()\n            # check if it is a file\n            self.logger.debug(\"path = %s\", path)\n            if os.path.isfile(path):\n                self.logger.debug(\"Add %s\", path)\n                list_with_files.append((\"\", path))\n\n            elif os.path.isdir(path):\n                for root, dir_names, filenames in os.walk(path):\n                    for file in filenames:\n                        full_file_path = os.path.join(root, file.strip(\"/\"))\n                        self.logger.debug(\"full_file_path = %s | dir_names = %s\", full_file_path, dir_names)\n                        if not os.path.isfile(full_file_path):\n                            continue\n                        self.logger.debug(\n                            \"get_basename(path) = %s | root = %s | replace = %s\",\n                            self.get_basename(path),\n                            root,\n                            root.replace(path, \"\"),\n                        )\n\n                        keep_path = os.path.join(self.get_basename(path), root.replace(path, \"\").strip(\"/\")).strip(\"/\")\n                        self.logger.debug(\"path = %s | keep_path = %s | root = %s\", path, keep_path, root)\n\n                        self.logger.debug(\"Add: %s | %s\", keep_path, full_file_path)\n                        list_with_files.append((keep_path, full_file_path))\n            else:\n                self.logger.warning(\"The path %s is not a directory nor a file, it will not be uploaded\", path)\n\n        return list_with_files\n\n    def list_s3_files_obj(self, bucket, prefix):\n        \"\"\"Retrieve the content of an S3 directory.\n\n        Args:\n            bucket (str): The S3 bucket name.\n            prefix (str): The S3 object key prefix.\n\n        Returns:\n            list: List containing S3 object keys.\n        \"\"\"\n\n        s3_files = []\n\n        try:\n            paginator: Any = self.s3_client.get_paginator(\"list_objects_v2\")\n            pages = paginator.paginate(Bucket=bucket, Prefix=prefix)\n            for page in pages:\n                for item in page.get(\"Contents\", ()):\n                    if item is not None:\n                        s3_files.append(item[\"Key\"])\n        except Exception as error:\n            self.logger.exception(f\"Exception when trying to list files from s3://{bucket}/{prefix}: {error}\")\n            raise RuntimeError(f\"Listing files from s3://{bucket}/{prefix}\") from error\n\n        return s3_files\n\n    def check_bucket_access(self, bucket):\n        \"\"\"Check the accessibility of an S3 bucket.\n\n        Args:\n            bucket (str): The S3 bucket name.\n\n        Raises:\n            RuntimeError: If an error occurs during the bucket access check.\n        \"\"\"\n\n        try:\n            self.connect_s3()\n            self.s3_client.head_bucket(Bucket=bucket)\n        except botocore.client.ClientError as error:\n            # check that it was a 404 vs 403 errors\n            # If it was a 404 error, then the bucket does not exist.\n            error_code = int(error.response[\"Error\"][\"Code\"])\n            if error_code == S3_ERR_FORBIDDEN_ACCESS:\n                self.logger.exception((f\"{bucket} is a private bucket. Forbidden access!\"))\n                raise RuntimeError(f\"{bucket} is a private bucket. Forbidden access!\") from error\n            if error_code == S3_ERR_NOT_FOUND:\n                self.logger.exception((f\"{bucket} bucket does not exist!\"))\n                raise RuntimeError(f\"{bucket} bucket does not exist!\") from error\n            self.logger.exception(f\"Exception when checking the access to {bucket} bucket: {error}\")\n            raise RuntimeError(f\"Exception when checking the access to {bucket} bucket\") from error\n        except botocore.exceptions.EndpointConnectionError as error:\n            self.logger.exception(f\"Could not connect to the endpoint when trying to access {bucket}: {error}\")\n            raise RuntimeError(f\"Could not connect to the endpoint when trying to access {bucket}!\") from error\n        except Exception as error:\n            self.logger.exception(f\"General exception when trying to access bucket {bucket}: {error}\")\n            raise RuntimeError(f\"General exception when trying to access bucket {bucket}\") from error\n\n    def wait_timeout(self, timeout):\n        \"\"\"\n        Wait for a specified timeout duration (minimum 200 ms).\n\n        This function implements a simple timeout mechanism, where it sleeps for 0.2 seconds\n        in each iteration until the cumulative sleep time reaches the specified timeout duration.\n\n        Args:\n            timeout (float): The total duration to wait in seconds.\n\n        Returns:\n            None\n\n        \"\"\"\n        time_cnt = 0.0\n        while time_cnt &lt; timeout:\n            time.sleep(SLEEP_TIME)\n            time_cnt += SLEEP_TIME\n\n    def check_file_overwriting(self, local_file, overwrite):\n        \"\"\"Check if file exists and determine if it should be overwritten.\n\n        Args:\n            local_file (str): Path to the local file.\n            overwrite (bool): Whether to overwrite the existing file.\n\n        Returns:\n            bool: True if the file should be overwritten, False otherwise.\n\n        Note:\n        - If the file already exists and the overwrite flag is set to True, the function logs a message,\n        deletes the existing file, and returns True.\n        - If the file already exists and the overwrite flag is set to False, the function logs a warning\n        message, and returns False. In this case, the existing file won't be deleted.\n        - If the file doesn't exist, the function returns True.\n\n        \"\"\"\n        if os.path.isfile(local_file):\n            if overwrite:  # The file already exists, so delete it first\n                self.logger.info(\n                    \"File %s already exists. Deleting it before downloading\",\n                    S3StorageHandler.get_basename(local_file),\n                )\n                os.remove(local_file)\n            else:\n                self.logger.warning(\n                    \"File %s already exists. Ignoring (use the overwrite flag if you want to overwrite this file)\",\n                    S3StorageHandler.get_basename(local_file),\n                )\n                return False\n\n        return True\n\n    def get_keys_from_s3(self, config: GetKeysFromS3Config) -&gt; list:\n        \"\"\"Download S3 keys specified in the configuration.\n\n        Args:\n            config (GetKeysFromS3Config): Configuration for the S3 download.\n\n        Returns:\n            List[str]: A list with the S3 keys that couldn't be downloaded.\n\n        Raises:\n            Exception: Any unexpected exception raised during the download process.\n\n        The function attempts to download files from S3 according to the provided configuration.\n        It returns a list of S3 keys that couldn't be downloaded successfully.\n\n        \"\"\"\n\n        # check the access to the bucket first, or even if it does exist\n        self.check_bucket_access(config.bucket)\n\n        # collection_files: list of files to be downloaded\n        #                   the list contains pair objects with the following\n        #                   syntax: (local_path_to_be_added_to_the_local_prefix, s3_key)\n        #                   the local_path_to_be_added_to_the_local_prefix may be none if the file doesn't exist\n        collection_files = self.files_to_be_downloaded(config.bucket, config.s3_files)\n\n        self.logger.debug(\"collection_files = %s | bucket = %s\", collection_files, config.bucket)\n        failed_files = []\n\n        for collection_file in collection_files:\n            if collection_file[0] is None:\n                failed_files.append(collection_file[1])\n                continue\n\n            local_path = os.path.join(config.local_prefix, collection_file[0].strip(\"/\"))\n            s3_file = collection_file[1]\n            # for each file to download, create the local dir (if it does not exist)\n            os.makedirs(local_path, exist_ok=True)\n            # create the path for local file\n            local_file = os.path.join(local_path, self.get_basename(s3_file).strip(\"/\"))\n\n            if not self.check_file_overwriting(local_file, config.overwrite):\n                continue\n            # download the files\n            downloaded = False\n            for keep_trying in range(config.max_retries):\n                try:\n                    self.connect_s3()\n                    dwn_start = datetime.now()\n                    self.s3_client.download_file(config.bucket, s3_file, local_file)\n                    self.logger.debug(\n                        \"s3://%s/%s downloaded to %s in %s ms\",\n                        config.bucket,\n                        s3_file,\n                        local_file,\n                        datetime.now() - dwn_start,\n                    )\n                    downloaded = True\n                    break\n                except (botocore.client.ClientError, botocore.exceptions.EndpointConnectionError) as error:\n                    self.logger.exception(\n                        \"Error when downloading the file %s. \\\nException: %s. Retrying in %s seconds for %s more times\",\n                        s3_file,\n                        error,\n                        DWN_S3FILE_RETRY_TIMEOUT,\n                        config.max_retries - keep_trying,\n                    )\n                    self.disconnect_s3()\n                    self.wait_timeout(DWN_S3FILE_RETRY_TIMEOUT)\n                except RuntimeError:\n                    self.logger.exception(\n                        \"Error when downloading the file %s. \\\nCouldn't get the s3 client. Retrying in %s seconds for %s more times\",\n                        s3_file,\n                        DWN_S3FILE_RETRY_TIMEOUT,\n                        config.max_retries - keep_trying,\n                    )\n                    self.wait_timeout(DWN_S3FILE_RETRY_TIMEOUT)\n\n            if not downloaded:\n                self.logger.error(\n                    \"Could not download the file %s. The download was \\\nretried for %s times. Aborting\",\n                    s3_file,\n                    config.max_retries,\n                )\n                failed_files.append(s3_file)\n\n        return failed_files\n\n    def put_files_to_s3(self, config: PutFilesToS3Config) -&gt; list:\n        \"\"\"Upload files to S3 according to the provided configuration.\n\n        Args:\n            config (PutFilesToS3Config): Configuration for the S3 upload.\n\n        Returns:\n            List[str]: A list with the local file paths that couldn't be uploaded.\n\n        Raises:\n            Exception: Any unexpected exception raised during the upload process.\n\n        The function attempts to upload files to S3 according to the provided configuration.\n        It returns a list of local files that couldn't be uploaded successfully.\n\n        \"\"\"\n\n        # check the access to the bucket first, or even if it does exist\n        self.check_bucket_access(config.bucket)\n\n        collection_files = self.files_to_be_uploaded(config.files)\n        failed_files = []\n\n        for collection_file in collection_files:\n            if collection_file[0] is None:\n                self.logger.error(\"The file %s can't be uploaded, its s3 prefix is None\", collection_file[0])\n                failed_files.append(collection_file[1])\n                continue\n\n            file_to_be_uploaded = collection_file[1]\n            # create the s3 key\n            s3_obj = os.path.join(config.s3_path, collection_file[0], os.path.basename(file_to_be_uploaded).strip(\"/\"))\n            uploaded = False\n            for keep_trying in range(config.max_retries):\n                try:\n                    # get the s3 client\n                    self.connect_s3()\n                    self.logger.info(\n                        \"Upload file %s to s3://%s/%s\",\n                        file_to_be_uploaded,\n                        config.bucket,\n                        s3_obj.lstrip(\"/\"),\n                    )\n\n                    self.s3_client.upload_file(file_to_be_uploaded, config.bucket, s3_obj)\n                    uploaded = True\n                    break\n                except (\n                    botocore.client.ClientError,\n                    botocore.exceptions.EndpointConnectionError,\n                    boto3.exceptions.S3UploadFailedError,\n                ) as error:\n                    self.logger.exception(\n                        \"Error when uploading the file %s. \\\nException: %s. Retrying in %s seconds for %s more times\",\n                        file_to_be_uploaded,\n                        error,\n                        UP_S3FILE_RETRY_TIMEOUT,\n                        config.max_retries - keep_trying,\n                    )\n                    self.disconnect_s3()\n                    self.wait_timeout(UP_S3FILE_RETRY_TIMEOUT)\n                except RuntimeError:\n                    self.logger.exception(\n                        \"Error when uploading the file %s. \\\nCouldn't get the s3 client. Retrying in %s seconds for %s more times\",\n                        file_to_be_uploaded,\n                        UP_S3FILE_RETRY_TIMEOUT,\n                        config.max_retries - keep_trying,\n                    )\n                    self.wait_timeout(UP_S3FILE_RETRY_TIMEOUT)\n\n            if not uploaded:\n                self.logger.error(\n                    \"Could not upload the file %s. The upload was \\\nretried for %s times. Aborting\",\n                    file_to_be_uploaded,\n                    config.max_retries,\n                )\n                failed_files.append(file_to_be_uploaded)\n\n        return failed_files\n\n    def transfer_from_s3_to_s3(self, config: TransferFromS3ToS3Config) -&gt; list:\n        \"\"\"Copy S3 keys specified in the configuration.\n        Args:\n            config (TransferFromS3ToS3Config): Configuration object containing bucket source, bucket destination,\n                      S3 files, maximum retries.\n\n        Returns:\n            list: A list of S3 keys that failed to be copied.\n\n        Raises:\n            Exception: Any unexpected exception raised during the upload process.\n        \"\"\"\n        # check the access to both buckets first, or even if they do exist\n        self.check_bucket_access(config.bucket_src)\n        self.check_bucket_access(config.bucket_dst)\n\n        # collection_files: list of files to be downloaded\n        #                   the list contains pair objects with the following\n        #                   syntax: (local_path_to_be_added_to_the_local_prefix, s3_key)\n\n        collection_files = self.files_to_be_downloaded(config.bucket_src, config.s3_files)\n\n        self.logger.debug(\"collection_files = %s | bucket = %s\", collection_files, config.bucket_src)\n        failed_files = []\n        copy_src = {\"Bucket\": config.bucket_src, \"Key\": \"\"}\n\n        for collection_file in collection_files:\n            if collection_file[0] is None:\n                failed_files.append(collection_file[1])\n                continue\n\n            copied = False\n            for keep_trying in range(config.max_retries):\n                self.logger.debug(\n                    \"keep_trying %s | range(config.max_retries) %s \",\n                    keep_trying,\n                    range(config.max_retries),\n                )\n                try:\n                    self.connect_s3()\n                    dwn_start = datetime.now()\n                    copy_src[\"Key\"] = collection_file[1]\n                    self.logger.debug(\"copy_src = %s\", copy_src)\n                    self.s3_client.copy_object(CopySource=copy_src, Bucket=config.bucket_dst, Key=collection_file[1])\n                    self.logger.debug(\n                        \"s3://%s/%s copied to s3://%s/%s in %s ms\",\n                        config.bucket_src,\n                        collection_file[1],\n                        config.bucket_dst,\n                        collection_file[1],\n                        datetime.now() - dwn_start,\n                    )\n                    if not config.copy_only:\n                        self.delete_file_from_s3(config.bucket_src, collection_file[1])\n                        self.logger.debug(\"Key deleted s3://%s/%s\", config.bucket_src, collection_file[1])\n                    copied = True\n                    break\n                except (botocore.client.ClientError, botocore.exceptions.EndpointConnectionError) as error:\n                    self.logger.exception(\n                        \"Error when copying the file s3://%s/%s to s3://%s. \\\nException: %s. Retrying in %s seconds for %s more times\",\n                        config.bucket_src,\n                        collection_file[1],\n                        config.bucket_dst,\n                        error,\n                        DWN_S3FILE_RETRY_TIMEOUT,\n                        config.max_retries - keep_trying,\n                    )\n                    self.disconnect_s3()\n                    self.wait_timeout(DWN_S3FILE_RETRY_TIMEOUT)\n                except RuntimeError:\n                    self.logger.exception(\n                        \"Error when copying the file s3://%s/%s to s3://%s. \\\nCouldn't get the s3 client. Retrying in %s seconds for %s more times\",\n                        config.bucket_src,\n                        collection_file[1],\n                        config.bucket_dst,\n                        DWN_S3FILE_RETRY_TIMEOUT,\n                        config.max_retries - keep_trying,\n                    )\n                    self.wait_timeout(DWN_S3FILE_RETRY_TIMEOUT)\n\n            if not copied:\n                self.logger.error(\n                    \"Could not copy the file s3://%s/%s to s3://%s. The copy was \\\nretried for %s times. Aborting\",\n                    config.bucket_src,\n                    collection_file[1],\n                    config.bucket_dst,\n                    config.max_retries,\n                )\n                failed_files.append(collection_file[1])\n\n        return failed_files\n</code></pre>"},{"location":"common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler.__get_s3_client","title":"<code>__get_s3_client(access_key_id, secret_access_key, endpoint_url, region_name)</code>","text":"<p>Retrieve or create an S3 client instance.</p> <p>Parameters:</p> Name Type Description Default <code>access_key_id</code> <code>str</code> <p>The access key ID for S3 authentication.</p> required <code>secret_access_key</code> <code>str</code> <p>The secret access key for S3 authentication.</p> required <code>endpoint_url</code> <code>str</code> <p>The endpoint URL for the S3 service.</p> required <code>region_name</code> <code>str</code> <p>The region name.</p> required <p>Returns:</p> Type Description <p>boto3.client: An S3 client instance.</p> Source code in <code>rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>def __get_s3_client(self, access_key_id, secret_access_key, endpoint_url, region_name):\n    \"\"\"Retrieve or create an S3 client instance.\n\n    Args:\n        access_key_id (str): The access key ID for S3 authentication.\n        secret_access_key (str): The secret access key for S3 authentication.\n        endpoint_url (str): The endpoint URL for the S3 service.\n        region_name (str): The region name.\n\n    Returns:\n        boto3.client: An S3 client instance.\n    \"\"\"\n\n    client_config = botocore.config.Config(\n        max_pool_connections=100,\n        # timeout for connection\n        connect_timeout=5,\n        # attempts in trying connection\n        # note:  the default behaviour of boto3 is retrying\n        # connections multiple times and exponentially backing off in between\n        retries={\"total_max_attempts\": 5},\n    )\n    try:\n        return boto3.client(\n            \"s3\",\n            aws_access_key_id=access_key_id,\n            aws_secret_access_key=secret_access_key,\n            endpoint_url=endpoint_url,\n            region_name=region_name,\n            config=client_config,\n        )\n\n    except Exception as e:\n        self.logger.exception(f\"Client error exception: {e}\")\n        raise RuntimeError(\"Client error exception \") from e\n</code></pre>"},{"location":"common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler.__init__","title":"<code>__init__(access_key_id, secret_access_key, endpoint_url, region_name)</code>","text":"<p>Initialize the S3StorageHandler instance.</p> <p>Parameters:</p> Name Type Description Default <code>access_key_id</code> <code>str</code> <p>The access key ID for S3 authentication.</p> required <code>secret_access_key</code> <code>str</code> <p>The secret access key for S3 authentication.</p> required <code>endpoint_url</code> <code>str</code> <p>The endpoint URL for the S3 service.</p> required <code>region_name</code> <code>str</code> <p>The region name.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the connection to the S3 storage cannot be established.</p> Source code in <code>rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>def __init__(self, access_key_id, secret_access_key, endpoint_url, region_name):\n    \"\"\"Initialize the S3StorageHandler instance.\n\n    Args:\n        access_key_id (str): The access key ID for S3 authentication.\n        secret_access_key (str): The secret access key for S3 authentication.\n        endpoint_url (str): The endpoint URL for the S3 service.\n        region_name (str): The region name.\n\n    Raises:\n        RuntimeError: If the connection to the S3 storage cannot be established.\n    \"\"\"\n    self.logger = Logging.default(__name__)\n\n    self.access_key_id = access_key_id\n    self.secret_access_key = secret_access_key\n    self.endpoint_url = endpoint_url\n    self.region_name = region_name\n    self.s3_client: boto3.client = None\n    self.connect_s3()\n    self.logger.debug(\"S3StorageHandler created !\")\n</code></pre>"},{"location":"common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler.check_bucket_access","title":"<code>check_bucket_access(bucket)</code>","text":"<p>Check the accessibility of an S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>The S3 bucket name.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If an error occurs during the bucket access check.</p> Source code in <code>rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>def check_bucket_access(self, bucket):\n    \"\"\"Check the accessibility of an S3 bucket.\n\n    Args:\n        bucket (str): The S3 bucket name.\n\n    Raises:\n        RuntimeError: If an error occurs during the bucket access check.\n    \"\"\"\n\n    try:\n        self.connect_s3()\n        self.s3_client.head_bucket(Bucket=bucket)\n    except botocore.client.ClientError as error:\n        # check that it was a 404 vs 403 errors\n        # If it was a 404 error, then the bucket does not exist.\n        error_code = int(error.response[\"Error\"][\"Code\"])\n        if error_code == S3_ERR_FORBIDDEN_ACCESS:\n            self.logger.exception((f\"{bucket} is a private bucket. Forbidden access!\"))\n            raise RuntimeError(f\"{bucket} is a private bucket. Forbidden access!\") from error\n        if error_code == S3_ERR_NOT_FOUND:\n            self.logger.exception((f\"{bucket} bucket does not exist!\"))\n            raise RuntimeError(f\"{bucket} bucket does not exist!\") from error\n        self.logger.exception(f\"Exception when checking the access to {bucket} bucket: {error}\")\n        raise RuntimeError(f\"Exception when checking the access to {bucket} bucket\") from error\n    except botocore.exceptions.EndpointConnectionError as error:\n        self.logger.exception(f\"Could not connect to the endpoint when trying to access {bucket}: {error}\")\n        raise RuntimeError(f\"Could not connect to the endpoint when trying to access {bucket}!\") from error\n    except Exception as error:\n        self.logger.exception(f\"General exception when trying to access bucket {bucket}: {error}\")\n        raise RuntimeError(f\"General exception when trying to access bucket {bucket}\") from error\n</code></pre>"},{"location":"common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler.check_file_overwriting","title":"<code>check_file_overwriting(local_file, overwrite)</code>","text":"<p>Check if file exists and determine if it should be overwritten.</p> <p>Parameters:</p> Name Type Description Default <code>local_file</code> <code>str</code> <p>Path to the local file.</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the existing file.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the file should be overwritten, False otherwise.</p> <p>Note: - If the file already exists and the overwrite flag is set to True, the function logs a message, deletes the existing file, and returns True. - If the file already exists and the overwrite flag is set to False, the function logs a warning message, and returns False. In this case, the existing file won't be deleted. - If the file doesn't exist, the function returns True.</p> Source code in <code>rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>def check_file_overwriting(self, local_file, overwrite):\n    \"\"\"Check if file exists and determine if it should be overwritten.\n\n    Args:\n        local_file (str): Path to the local file.\n        overwrite (bool): Whether to overwrite the existing file.\n\n    Returns:\n        bool: True if the file should be overwritten, False otherwise.\n\n    Note:\n    - If the file already exists and the overwrite flag is set to True, the function logs a message,\n    deletes the existing file, and returns True.\n    - If the file already exists and the overwrite flag is set to False, the function logs a warning\n    message, and returns False. In this case, the existing file won't be deleted.\n    - If the file doesn't exist, the function returns True.\n\n    \"\"\"\n    if os.path.isfile(local_file):\n        if overwrite:  # The file already exists, so delete it first\n            self.logger.info(\n                \"File %s already exists. Deleting it before downloading\",\n                S3StorageHandler.get_basename(local_file),\n            )\n            os.remove(local_file)\n        else:\n            self.logger.warning(\n                \"File %s already exists. Ignoring (use the overwrite flag if you want to overwrite this file)\",\n                S3StorageHandler.get_basename(local_file),\n            )\n            return False\n\n    return True\n</code></pre>"},{"location":"common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler.connect_s3","title":"<code>connect_s3()</code>","text":"<p>Establish a connection to the S3 service.</p> <p>If the S3 client is not already instantiated, this method calls the private get_s3_client method to create an S3 client instance using the provided credentials and configuration (see __init).</p> Source code in <code>rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>def connect_s3(self):\n    \"\"\"Establish a connection to the S3 service.\n\n    If the S3 client is not already instantiated, this method calls the private __get_s3_client\n    method to create an S3 client instance using the provided credentials and configuration (see __init__).\n    \"\"\"\n    if self.s3_client is None:\n        self.s3_client = self.__get_s3_client(\n            self.access_key_id,\n            self.secret_access_key,\n            self.endpoint_url,\n            self.region_name,\n        )\n</code></pre>"},{"location":"common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler.delete_file_from_s3","title":"<code>delete_file_from_s3(bucket, s3_obj)</code>","text":"<p>Delete a file from S3.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>The S3 bucket name.</p> required <code>s3_obj</code> <code>str</code> <p>The S3 object key.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If an error occurs during the bucket access check.</p> Source code in <code>rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>def delete_file_from_s3(self, bucket, s3_obj):\n    \"\"\"Delete a file from S3.\n\n    Args:\n        bucket (str): The S3 bucket name.\n        s3_obj (str): The S3 object key.\n\n    Raises:\n        RuntimeError: If an error occurs during the bucket access check.\n    \"\"\"\n    if self.s3_client is None or bucket is None or s3_obj is None:\n        raise RuntimeError(\"Input error for deleting the file\")\n    try:\n        self.logger.info(\"Delete key s3://%s/%s\", bucket, s3_obj)\n        self.s3_client.delete_object(Bucket=bucket, Key=s3_obj)\n    except botocore.client.ClientError as e:\n        self.logger.exception(f\"Failed to delete key s3://{bucket}/{s3_obj}: {e}\")\n        raise RuntimeError(f\"Failed to delete key s3://{bucket}/{s3_obj}\") from e\n    except Exception as e:\n        self.logger.exception(f\"Failed to delete key s3://{bucket}/{s3_obj}: {e}\")\n        raise RuntimeError(f\"Failed to delete key s3://{bucket}/{s3_obj}\") from e\n</code></pre>"},{"location":"common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler.disconnect_s3","title":"<code>disconnect_s3()</code>","text":"<p>Close the connection to the S3 service.</p> Source code in <code>rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>def disconnect_s3(self):\n    \"\"\"Close the connection to the S3 service.\"\"\"\n    if self.s3_client is None:\n        return\n    self.s3_client.close()\n    self.s3_client = None\n</code></pre>"},{"location":"common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler.files_to_be_downloaded","title":"<code>files_to_be_downloaded(bucket, paths)</code>","text":"<p>Create a list with the S3 keys to be downloaded.</p> <p>The list will have the s3 keys to be downloaded from the bucket. It contains pairs (local_prefix_where_the_file_will_be_downloaded, full_s3_key_path) If a s3 key doesn't exist, the pair will be (None, requested_s3_key_path)</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>The S3 bucket name.</p> required <code>paths</code> <code>list</code> <p>List of S3 object keys.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>List of tuples (local_prefix, full_s3_key_path).</p> Source code in <code>rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>def files_to_be_downloaded(self, bucket, paths):\n    \"\"\"Create a list with the S3 keys to be downloaded.\n\n    The list will have the s3 keys to be downloaded from the bucket.\n    It contains pairs (local_prefix_where_the_file_will_be_downloaded, full_s3_key_path)\n    If a s3 key doesn't exist, the pair will be (None, requested_s3_key_path)\n\n    Args:\n        bucket (str): The S3 bucket name.\n        paths (list): List of S3 object keys.\n\n    Returns:\n        list: List of tuples (local_prefix, full_s3_key_path).\n    \"\"\"\n    # declaration of the list\n    list_with_files: List[Any] = []\n    # for each key, identify it as a file or a folder\n    # in the case of a folder, the files will be recursively gathered\n    for key in paths:\n        path = key.strip().lstrip(\"/\")\n        s3_files = self.list_s3_files_obj(bucket, path)\n        if len(s3_files) == 0:\n            self.logger.warning(\"No key %s found.\", path)\n            list_with_files.append((None, path))\n            continue\n        self.logger.debug(\"total: %s | s3_files = %s\", len(s3_files), s3_files)\n        basename_part = self.get_basename(path)\n\n        # check if it's a file or a dir\n        if len(s3_files) == 1 and path == s3_files[0]:\n            # the current key is a file, append it to the list\n            list_with_files.append((\"\", s3_files[0]))\n            self.logger.debug(\"Append files: list_with_files = %s\", list_with_files)\n        else:\n            # the current key is a folder, append all its files (reursively gathered) to the list\n            for s3_file in s3_files:\n                split = s3_file.split(\"/\")\n                split_idx = split.index(basename_part)\n                list_with_files.append((os.path.join(*split[split_idx:-1]), s3_file.strip(\"/\")))\n\n    return list_with_files\n</code></pre>"},{"location":"common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler.files_to_be_uploaded","title":"<code>files_to_be_uploaded(paths)</code>","text":"<p>Creates a list with the local files to be uploaded.</p> <p>The list contains pairs (s3_path, absolute_local_file_path) If the local file doesn't exist, the pair will be (None, requested_file_to_upload)</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>list</code> <p>List of local file paths.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>List of tuples (s3_path, absolute_local_file_path).</p> Source code in <code>rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>def files_to_be_uploaded(self, paths):\n    \"\"\"Creates a list with the local files to be uploaded.\n\n    The list contains pairs (s3_path, absolute_local_file_path)\n    If the local file doesn't exist, the pair will be (None, requested_file_to_upload)\n\n    Args:\n        paths (list): List of local file paths.\n\n    Returns:\n        list: List of tuples (s3_path, absolute_local_file_path).\n    \"\"\"\n\n    list_with_files = []\n    for local in paths:\n        path = local.strip()\n        # check if it is a file\n        self.logger.debug(\"path = %s\", path)\n        if os.path.isfile(path):\n            self.logger.debug(\"Add %s\", path)\n            list_with_files.append((\"\", path))\n\n        elif os.path.isdir(path):\n            for root, dir_names, filenames in os.walk(path):\n                for file in filenames:\n                    full_file_path = os.path.join(root, file.strip(\"/\"))\n                    self.logger.debug(\"full_file_path = %s | dir_names = %s\", full_file_path, dir_names)\n                    if not os.path.isfile(full_file_path):\n                        continue\n                    self.logger.debug(\n                        \"get_basename(path) = %s | root = %s | replace = %s\",\n                        self.get_basename(path),\n                        root,\n                        root.replace(path, \"\"),\n                    )\n\n                    keep_path = os.path.join(self.get_basename(path), root.replace(path, \"\").strip(\"/\")).strip(\"/\")\n                    self.logger.debug(\"path = %s | keep_path = %s | root = %s\", path, keep_path, root)\n\n                    self.logger.debug(\"Add: %s | %s\", keep_path, full_file_path)\n                    list_with_files.append((keep_path, full_file_path))\n        else:\n            self.logger.warning(\"The path %s is not a directory nor a file, it will not be uploaded\", path)\n\n    return list_with_files\n</code></pre>"},{"location":"common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler.get_basename","title":"<code>get_basename(input_path)</code>  <code>staticmethod</code>","text":"<p>Get the filename from a full path.</p> <p>Parameters:</p> Name Type Description Default <code>int_path</code> <code>str</code> <p>The full path.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The filename.</p> Source code in <code>rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>@staticmethod\ndef get_basename(input_path):\n    \"\"\"Get the filename from a full path.\n\n    Args:\n        int_path (str): The full path.\n\n    Returns:\n        str: The filename.\n    \"\"\"\n    path, filename = ntpath.split(input_path)\n    return filename or ntpath.basename(path)\n</code></pre>"},{"location":"common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler.get_keys_from_s3","title":"<code>get_keys_from_s3(config)</code>","text":"<p>Download S3 keys specified in the configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>GetKeysFromS3Config</code> <p>Configuration for the S3 download.</p> required <p>Returns:</p> Type Description <code>list</code> <p>List[str]: A list with the S3 keys that couldn't be downloaded.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Any unexpected exception raised during the download process.</p> <p>The function attempts to download files from S3 according to the provided configuration. It returns a list of S3 keys that couldn't be downloaded successfully.</p> Source code in <code>rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>    def get_keys_from_s3(self, config: GetKeysFromS3Config) -&gt; list:\n        \"\"\"Download S3 keys specified in the configuration.\n\n        Args:\n            config (GetKeysFromS3Config): Configuration for the S3 download.\n\n        Returns:\n            List[str]: A list with the S3 keys that couldn't be downloaded.\n\n        Raises:\n            Exception: Any unexpected exception raised during the download process.\n\n        The function attempts to download files from S3 according to the provided configuration.\n        It returns a list of S3 keys that couldn't be downloaded successfully.\n\n        \"\"\"\n\n        # check the access to the bucket first, or even if it does exist\n        self.check_bucket_access(config.bucket)\n\n        # collection_files: list of files to be downloaded\n        #                   the list contains pair objects with the following\n        #                   syntax: (local_path_to_be_added_to_the_local_prefix, s3_key)\n        #                   the local_path_to_be_added_to_the_local_prefix may be none if the file doesn't exist\n        collection_files = self.files_to_be_downloaded(config.bucket, config.s3_files)\n\n        self.logger.debug(\"collection_files = %s | bucket = %s\", collection_files, config.bucket)\n        failed_files = []\n\n        for collection_file in collection_files:\n            if collection_file[0] is None:\n                failed_files.append(collection_file[1])\n                continue\n\n            local_path = os.path.join(config.local_prefix, collection_file[0].strip(\"/\"))\n            s3_file = collection_file[1]\n            # for each file to download, create the local dir (if it does not exist)\n            os.makedirs(local_path, exist_ok=True)\n            # create the path for local file\n            local_file = os.path.join(local_path, self.get_basename(s3_file).strip(\"/\"))\n\n            if not self.check_file_overwriting(local_file, config.overwrite):\n                continue\n            # download the files\n            downloaded = False\n            for keep_trying in range(config.max_retries):\n                try:\n                    self.connect_s3()\n                    dwn_start = datetime.now()\n                    self.s3_client.download_file(config.bucket, s3_file, local_file)\n                    self.logger.debug(\n                        \"s3://%s/%s downloaded to %s in %s ms\",\n                        config.bucket,\n                        s3_file,\n                        local_file,\n                        datetime.now() - dwn_start,\n                    )\n                    downloaded = True\n                    break\n                except (botocore.client.ClientError, botocore.exceptions.EndpointConnectionError) as error:\n                    self.logger.exception(\n                        \"Error when downloading the file %s. \\\nException: %s. Retrying in %s seconds for %s more times\",\n                        s3_file,\n                        error,\n                        DWN_S3FILE_RETRY_TIMEOUT,\n                        config.max_retries - keep_trying,\n                    )\n                    self.disconnect_s3()\n                    self.wait_timeout(DWN_S3FILE_RETRY_TIMEOUT)\n                except RuntimeError:\n                    self.logger.exception(\n                        \"Error when downloading the file %s. \\\nCouldn't get the s3 client. Retrying in %s seconds for %s more times\",\n                        s3_file,\n                        DWN_S3FILE_RETRY_TIMEOUT,\n                        config.max_retries - keep_trying,\n                    )\n                    self.wait_timeout(DWN_S3FILE_RETRY_TIMEOUT)\n\n            if not downloaded:\n                self.logger.error(\n                    \"Could not download the file %s. The download was \\\nretried for %s times. Aborting\",\n                    s3_file,\n                    config.max_retries,\n                )\n                failed_files.append(s3_file)\n\n        return failed_files\n</code></pre>"},{"location":"common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler.get_secrets_from_file","title":"<code>get_secrets_from_file(secrets, secret_file)</code>  <code>staticmethod</code>","text":"<p>Read secrets from a specified file.</p> <p>It reads the secrets from .s3cfg or aws credentials files This function should not be used in production</p> <p>Parameters:</p> Name Type Description Default <code>secrets</code> <code>dict</code> <p>Dictionary to store retrieved secrets.</p> required <code>secret_file</code> <code>str</code> <p>Path to the file containing secrets.</p> required <code>logger</code> <code>Logger</code> <p>Logger instance for error logging.</p> required Source code in <code>rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>@staticmethod\ndef get_secrets_from_file(secrets, secret_file):\n    \"\"\"Read secrets from a specified file.\n\n    It reads the secrets from .s3cfg or aws credentials files\n    This function should not be used in production\n\n    Args:\n        secrets (dict): Dictionary to store retrieved secrets.\n        secret_file (str): Path to the file containing secrets.\n        logger (Logger, optional): Logger instance for error logging.\n    \"\"\"\n    dict_filled = 0\n    with open(secret_file, \"r\", encoding=\"utf-8\") as aws_credentials_file:\n        lines = aws_credentials_file.readlines()\n        for line in lines:\n            if not secrets[\"s3endpoint\"] and \"host_bucket\" in line:\n                dict_filled += 1\n                secrets[\"s3endpoint\"] = line.strip().split(\"=\")[1].strip()\n            elif not secrets[\"accesskey\"] and \"access_key\" in line:\n                dict_filled += 1\n                secrets[\"accesskey\"] = line.strip().split(\"=\")[1].strip()\n            elif not secrets[\"secretkey\"] and \"secret_\" in line and \"_key\" in line:\n                dict_filled += 1\n                secrets[\"secretkey\"] = line.strip().split(\"=\")[1].strip()\n            if dict_filled == 3:\n                break\n</code></pre>"},{"location":"common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler.list_s3_files_obj","title":"<code>list_s3_files_obj(bucket, prefix)</code>","text":"<p>Retrieve the content of an S3 directory.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>The S3 bucket name.</p> required <code>prefix</code> <code>str</code> <p>The S3 object key prefix.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>List containing S3 object keys.</p> Source code in <code>rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>def list_s3_files_obj(self, bucket, prefix):\n    \"\"\"Retrieve the content of an S3 directory.\n\n    Args:\n        bucket (str): The S3 bucket name.\n        prefix (str): The S3 object key prefix.\n\n    Returns:\n        list: List containing S3 object keys.\n    \"\"\"\n\n    s3_files = []\n\n    try:\n        paginator: Any = self.s3_client.get_paginator(\"list_objects_v2\")\n        pages = paginator.paginate(Bucket=bucket, Prefix=prefix)\n        for page in pages:\n            for item in page.get(\"Contents\", ()):\n                if item is not None:\n                    s3_files.append(item[\"Key\"])\n    except Exception as error:\n        self.logger.exception(f\"Exception when trying to list files from s3://{bucket}/{prefix}: {error}\")\n        raise RuntimeError(f\"Listing files from s3://{bucket}/{prefix}\") from error\n\n    return s3_files\n</code></pre>"},{"location":"common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler.put_files_to_s3","title":"<code>put_files_to_s3(config)</code>","text":"<p>Upload files to S3 according to the provided configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>PutFilesToS3Config</code> <p>Configuration for the S3 upload.</p> required <p>Returns:</p> Type Description <code>list</code> <p>List[str]: A list with the local file paths that couldn't be uploaded.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Any unexpected exception raised during the upload process.</p> <p>The function attempts to upload files to S3 according to the provided configuration. It returns a list of local files that couldn't be uploaded successfully.</p> Source code in <code>rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>    def put_files_to_s3(self, config: PutFilesToS3Config) -&gt; list:\n        \"\"\"Upload files to S3 according to the provided configuration.\n\n        Args:\n            config (PutFilesToS3Config): Configuration for the S3 upload.\n\n        Returns:\n            List[str]: A list with the local file paths that couldn't be uploaded.\n\n        Raises:\n            Exception: Any unexpected exception raised during the upload process.\n\n        The function attempts to upload files to S3 according to the provided configuration.\n        It returns a list of local files that couldn't be uploaded successfully.\n\n        \"\"\"\n\n        # check the access to the bucket first, or even if it does exist\n        self.check_bucket_access(config.bucket)\n\n        collection_files = self.files_to_be_uploaded(config.files)\n        failed_files = []\n\n        for collection_file in collection_files:\n            if collection_file[0] is None:\n                self.logger.error(\"The file %s can't be uploaded, its s3 prefix is None\", collection_file[0])\n                failed_files.append(collection_file[1])\n                continue\n\n            file_to_be_uploaded = collection_file[1]\n            # create the s3 key\n            s3_obj = os.path.join(config.s3_path, collection_file[0], os.path.basename(file_to_be_uploaded).strip(\"/\"))\n            uploaded = False\n            for keep_trying in range(config.max_retries):\n                try:\n                    # get the s3 client\n                    self.connect_s3()\n                    self.logger.info(\n                        \"Upload file %s to s3://%s/%s\",\n                        file_to_be_uploaded,\n                        config.bucket,\n                        s3_obj.lstrip(\"/\"),\n                    )\n\n                    self.s3_client.upload_file(file_to_be_uploaded, config.bucket, s3_obj)\n                    uploaded = True\n                    break\n                except (\n                    botocore.client.ClientError,\n                    botocore.exceptions.EndpointConnectionError,\n                    boto3.exceptions.S3UploadFailedError,\n                ) as error:\n                    self.logger.exception(\n                        \"Error when uploading the file %s. \\\nException: %s. Retrying in %s seconds for %s more times\",\n                        file_to_be_uploaded,\n                        error,\n                        UP_S3FILE_RETRY_TIMEOUT,\n                        config.max_retries - keep_trying,\n                    )\n                    self.disconnect_s3()\n                    self.wait_timeout(UP_S3FILE_RETRY_TIMEOUT)\n                except RuntimeError:\n                    self.logger.exception(\n                        \"Error when uploading the file %s. \\\nCouldn't get the s3 client. Retrying in %s seconds for %s more times\",\n                        file_to_be_uploaded,\n                        UP_S3FILE_RETRY_TIMEOUT,\n                        config.max_retries - keep_trying,\n                    )\n                    self.wait_timeout(UP_S3FILE_RETRY_TIMEOUT)\n\n            if not uploaded:\n                self.logger.error(\n                    \"Could not upload the file %s. The upload was \\\nretried for %s times. Aborting\",\n                    file_to_be_uploaded,\n                    config.max_retries,\n                )\n                failed_files.append(file_to_be_uploaded)\n\n        return failed_files\n</code></pre>"},{"location":"common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler.s3_path_parser","title":"<code>s3_path_parser(s3_url)</code>  <code>staticmethod</code>","text":"<p>Parses S3 URL to extract bucket, prefix, and file.</p> <p>Parameters:</p> Name Type Description Default <code>s3_url</code> <code>str</code> <p>The S3 URL.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>Tuple containing bucket, prefix, and file.</p> Source code in <code>rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>@staticmethod\ndef s3_path_parser(s3_url):\n    \"\"\"\n    Parses S3 URL to extract bucket, prefix, and file.\n\n    Args:\n        s3_url (str): The S3 URL.\n\n    Returns:\n        tuple: Tuple containing bucket, prefix, and file.\n    \"\"\"\n    s3_data = s3_url.replace(\"s3://\", \"\").split(\"/\")\n    bucket = \"\"\n    start_idx = 0\n    if s3_url.startswith(\"s3://\"):\n        bucket = s3_data[0]\n\n        start_idx = 1\n    prefix = \"\"\n    if start_idx &lt; len(s3_data):\n        prefix = \"/\".join(s3_data[start_idx:-1])\n    s3_file = s3_data[-1]\n    return bucket, prefix, s3_file\n</code></pre>"},{"location":"common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler.transfer_from_s3_to_s3","title":"<code>transfer_from_s3_to_s3(config)</code>","text":"<p>Copy S3 keys specified in the configuration. Args:     config (TransferFromS3ToS3Config): Configuration object containing bucket source, bucket destination,               S3 files, maximum retries.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of S3 keys that failed to be copied.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Any unexpected exception raised during the upload process.</p> Source code in <code>rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>    def transfer_from_s3_to_s3(self, config: TransferFromS3ToS3Config) -&gt; list:\n        \"\"\"Copy S3 keys specified in the configuration.\n        Args:\n            config (TransferFromS3ToS3Config): Configuration object containing bucket source, bucket destination,\n                      S3 files, maximum retries.\n\n        Returns:\n            list: A list of S3 keys that failed to be copied.\n\n        Raises:\n            Exception: Any unexpected exception raised during the upload process.\n        \"\"\"\n        # check the access to both buckets first, or even if they do exist\n        self.check_bucket_access(config.bucket_src)\n        self.check_bucket_access(config.bucket_dst)\n\n        # collection_files: list of files to be downloaded\n        #                   the list contains pair objects with the following\n        #                   syntax: (local_path_to_be_added_to_the_local_prefix, s3_key)\n\n        collection_files = self.files_to_be_downloaded(config.bucket_src, config.s3_files)\n\n        self.logger.debug(\"collection_files = %s | bucket = %s\", collection_files, config.bucket_src)\n        failed_files = []\n        copy_src = {\"Bucket\": config.bucket_src, \"Key\": \"\"}\n\n        for collection_file in collection_files:\n            if collection_file[0] is None:\n                failed_files.append(collection_file[1])\n                continue\n\n            copied = False\n            for keep_trying in range(config.max_retries):\n                self.logger.debug(\n                    \"keep_trying %s | range(config.max_retries) %s \",\n                    keep_trying,\n                    range(config.max_retries),\n                )\n                try:\n                    self.connect_s3()\n                    dwn_start = datetime.now()\n                    copy_src[\"Key\"] = collection_file[1]\n                    self.logger.debug(\"copy_src = %s\", copy_src)\n                    self.s3_client.copy_object(CopySource=copy_src, Bucket=config.bucket_dst, Key=collection_file[1])\n                    self.logger.debug(\n                        \"s3://%s/%s copied to s3://%s/%s in %s ms\",\n                        config.bucket_src,\n                        collection_file[1],\n                        config.bucket_dst,\n                        collection_file[1],\n                        datetime.now() - dwn_start,\n                    )\n                    if not config.copy_only:\n                        self.delete_file_from_s3(config.bucket_src, collection_file[1])\n                        self.logger.debug(\"Key deleted s3://%s/%s\", config.bucket_src, collection_file[1])\n                    copied = True\n                    break\n                except (botocore.client.ClientError, botocore.exceptions.EndpointConnectionError) as error:\n                    self.logger.exception(\n                        \"Error when copying the file s3://%s/%s to s3://%s. \\\nException: %s. Retrying in %s seconds for %s more times\",\n                        config.bucket_src,\n                        collection_file[1],\n                        config.bucket_dst,\n                        error,\n                        DWN_S3FILE_RETRY_TIMEOUT,\n                        config.max_retries - keep_trying,\n                    )\n                    self.disconnect_s3()\n                    self.wait_timeout(DWN_S3FILE_RETRY_TIMEOUT)\n                except RuntimeError:\n                    self.logger.exception(\n                        \"Error when copying the file s3://%s/%s to s3://%s. \\\nCouldn't get the s3 client. Retrying in %s seconds for %s more times\",\n                        config.bucket_src,\n                        collection_file[1],\n                        config.bucket_dst,\n                        DWN_S3FILE_RETRY_TIMEOUT,\n                        config.max_retries - keep_trying,\n                    )\n                    self.wait_timeout(DWN_S3FILE_RETRY_TIMEOUT)\n\n            if not copied:\n                self.logger.error(\n                    \"Could not copy the file s3://%s/%s to s3://%s. The copy was \\\nretried for %s times. Aborting\",\n                    config.bucket_src,\n                    collection_file[1],\n                    config.bucket_dst,\n                    config.max_retries,\n                )\n                failed_files.append(collection_file[1])\n\n        return failed_files\n</code></pre>"},{"location":"common/#rs_server_common.s3_storage_handler.s3_storage_handler.S3StorageHandler.wait_timeout","title":"<code>wait_timeout(timeout)</code>","text":"<p>Wait for a specified timeout duration (minimum 200 ms).</p> <p>This function implements a simple timeout mechanism, where it sleeps for 0.2 seconds in each iteration until the cumulative sleep time reaches the specified timeout duration.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>The total duration to wait in seconds.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>def wait_timeout(self, timeout):\n    \"\"\"\n    Wait for a specified timeout duration (minimum 200 ms).\n\n    This function implements a simple timeout mechanism, where it sleeps for 0.2 seconds\n    in each iteration until the cumulative sleep time reaches the specified timeout duration.\n\n    Args:\n        timeout (float): The total duration to wait in seconds.\n\n    Returns:\n        None\n\n    \"\"\"\n    time_cnt = 0.0\n    while time_cnt &lt; timeout:\n        time.sleep(SLEEP_TIME)\n        time_cnt += SLEEP_TIME\n</code></pre>"},{"location":"common/#rs_server_common.s3_storage_handler.s3_storage_handler.TransferFromS3ToS3Config","title":"<code>TransferFromS3ToS3Config</code>  <code>dataclass</code>","text":"<p>S3 configuration for copying a list with keys between buckets</p> <p>Attributes:</p> Name Type Description <code>s3_files</code> <code>list</code> <p>A list with the S3 object keys to be copied.</p> <code>bucket_src</code> <code>str</code> <p>The source S3 bucket name.</p> <code>bucket_dst</code> <code>str</code> <p>The destination S3 bucket name.</p> <code>max_retries</code> <code>int</code> <p>The maximum number of download retries. Default is DWN_S3FILE_RETRIES.</p> Source code in <code>rs_server_common/s3_storage_handler/s3_storage_handler.py</code> <pre><code>@dataclass\nclass TransferFromS3ToS3Config:\n    \"\"\"S3 configuration for copying a list with keys between buckets\n\n    Attributes:\n        s3_files (list): A list with the S3 object keys to be copied.\n        bucket_src (str): The source S3 bucket name.\n        bucket_dst (str): The destination S3 bucket name.\n        max_retries (int, optional): The maximum number of download retries. Default is DWN_S3FILE_RETRIES.\n\n    \"\"\"\n\n    s3_files: list\n    bucket_src: str\n    bucket_dst: str\n    copy_only: bool = False\n    max_retries: int = DWN_S3FILE_RETRIES\n</code></pre>"},{"location":"frontend/","title":"Frontend","text":"<p>The frontend application.</p>"},{"location":"frontend/#rs_server_frontend.main.Frontend","title":"<code>Frontend</code>","text":"<p>The frontend application.</p> Source code in <code>rs_server_frontend/main.py</code> <pre><code>class Frontend:\n    \"\"\"The frontend application.\"\"\"\n\n    def __init__(self):\n        \"\"\"Create a frontend application.\n\n        The frontend serves the rs-server REST API documentation.\n        This documentation is an openapi specification loaded from a json file.\n        This file location is given by the RSPY_OPENAPI_FILE environment variable.\n\n        This file is loaded during the frontend application initialization\n        and is kept in memory cache for the entire life of the application.\n\n        A specific FrontendFailed exception is raised if the openapi loading failed.\n        \"\"\"\n\n        # For cluster deployment: override the swagger /docs URL from an environment variable.\n        # Also set the openapi.json URL under the same path.\n        try:\n            docs_url = env[\"RSPY_DOCS_URL\"].strip(\"/\")\n            docs_params = {\"docs_url\": f\"/{docs_url}\", \"openapi_url\": f\"/{docs_url}/openapi.json\"}\n        except KeyError:\n            docs_params = {}\n\n        try:\n            self.openapi_spec: dict = self.load_openapi_spec()\n            self.app: FastAPI = FastAPI(\n                version=__version__,\n                **docs_params,  # type: ignore\n                # Same hardcoded values than in the apikey manager\n                # (they don't appear in the openapi.json)\n                swagger_ui_init_oauth={\n                    \"clientId\": \"(this value is not used)\",\n                    \"appName\": \"API-Key Manager\",\n                    \"usePkceWithAuthorizationCodeGrant\": True,\n                },\n            )\n            self.app.openapi = self.get_openapi  # type: ignore\n        except BaseException as e:\n            raise FrontendFailed(\"Unable to serve openapi specification.\") from e\n\n        # include_in_schema=False: hide this endpoint from the swagger\n        @self.app.get(\"/health\", response_model=HealthSchema, name=\"Check service health\", include_in_schema=False)\n        async def health() -&gt; HealthSchema:\n            \"\"\"\n            Always return a flag set to 'true' when the service is up and running.\n            \\f\n            Otherwise this code won't be run anyway and the caller will have other sorts of errors.\n            \"\"\"\n            return HealthSchema(healthy=True)\n\n    @staticmethod\n    def load_openapi_spec() -&gt; dict:\n        \"\"\"Load the openapi specification.\n\n        The openapi is loaded from a json file.\n        This json file location is given by the environment variable RSPY_OPENAPI_FILE.\n\n        An IOError is raised in case of errors during the file reading.\n        A ValueError is raised in case of errors during the json parsing.\n\n        Returns:\n            the loaded openapi specification\n\n        \"\"\"\n        openapi_location = os.getenv(\"RSPY_OPENAPI_FILE\", \"\")\n        try:\n            with open(openapi_location, \"r\", encoding=\"utf-8\") as file:\n                return json.load(file)\n        except (FileNotFoundError, IOError) as e:\n            raise type(e)(\n                f\"openapi spec was not found at {openapi_location!r}. \"\n                \"Is the 'RSPY_OPENAPI_FILE' environment variable correctly set ?\",\n            ) from e\n        except ValueError as e:\n            raise ValueError(\n                f\"openapi spec was found at {openapi_location!r} but the file is not valid.\",\n            ) from e\n\n    def get_openapi(self) -&gt; dict:\n        \"\"\"Returns the openapi specification.\n\n        Returns:\n            the openapi specification as a dict.\n        \"\"\"\n        return self.openapi_spec\n</code></pre>"},{"location":"frontend/#rs_server_frontend.main.Frontend.__init__","title":"<code>__init__()</code>","text":"<p>Create a frontend application.</p> <p>The frontend serves the rs-server REST API documentation. This documentation is an openapi specification loaded from a json file. This file location is given by the RSPY_OPENAPI_FILE environment variable.</p> <p>This file is loaded during the frontend application initialization and is kept in memory cache for the entire life of the application.</p> <p>A specific FrontendFailed exception is raised if the openapi loading failed.</p> Source code in <code>rs_server_frontend/main.py</code> <pre><code>def __init__(self):\n    \"\"\"Create a frontend application.\n\n    The frontend serves the rs-server REST API documentation.\n    This documentation is an openapi specification loaded from a json file.\n    This file location is given by the RSPY_OPENAPI_FILE environment variable.\n\n    This file is loaded during the frontend application initialization\n    and is kept in memory cache for the entire life of the application.\n\n    A specific FrontendFailed exception is raised if the openapi loading failed.\n    \"\"\"\n\n    # For cluster deployment: override the swagger /docs URL from an environment variable.\n    # Also set the openapi.json URL under the same path.\n    try:\n        docs_url = env[\"RSPY_DOCS_URL\"].strip(\"/\")\n        docs_params = {\"docs_url\": f\"/{docs_url}\", \"openapi_url\": f\"/{docs_url}/openapi.json\"}\n    except KeyError:\n        docs_params = {}\n\n    try:\n        self.openapi_spec: dict = self.load_openapi_spec()\n        self.app: FastAPI = FastAPI(\n            version=__version__,\n            **docs_params,  # type: ignore\n            # Same hardcoded values than in the apikey manager\n            # (they don't appear in the openapi.json)\n            swagger_ui_init_oauth={\n                \"clientId\": \"(this value is not used)\",\n                \"appName\": \"API-Key Manager\",\n                \"usePkceWithAuthorizationCodeGrant\": True,\n            },\n        )\n        self.app.openapi = self.get_openapi  # type: ignore\n    except BaseException as e:\n        raise FrontendFailed(\"Unable to serve openapi specification.\") from e\n\n    # include_in_schema=False: hide this endpoint from the swagger\n    @self.app.get(\"/health\", response_model=HealthSchema, name=\"Check service health\", include_in_schema=False)\n    async def health() -&gt; HealthSchema:\n        \"\"\"\n        Always return a flag set to 'true' when the service is up and running.\n        \\f\n        Otherwise this code won't be run anyway and the caller will have other sorts of errors.\n        \"\"\"\n        return HealthSchema(healthy=True)\n</code></pre>"},{"location":"frontend/#rs_server_frontend.main.Frontend.get_openapi","title":"<code>get_openapi()</code>","text":"<p>Returns the openapi specification.</p> <p>Returns:</p> Type Description <code>dict</code> <p>the openapi specification as a dict.</p> Source code in <code>rs_server_frontend/main.py</code> <pre><code>def get_openapi(self) -&gt; dict:\n    \"\"\"Returns the openapi specification.\n\n    Returns:\n        the openapi specification as a dict.\n    \"\"\"\n    return self.openapi_spec\n</code></pre>"},{"location":"frontend/#rs_server_frontend.main.Frontend.load_openapi_spec","title":"<code>load_openapi_spec()</code>  <code>staticmethod</code>","text":"<p>Load the openapi specification.</p> <p>The openapi is loaded from a json file. This json file location is given by the environment variable RSPY_OPENAPI_FILE.</p> <p>An IOError is raised in case of errors during the file reading. A ValueError is raised in case of errors during the json parsing.</p> <p>Returns:</p> Type Description <code>dict</code> <p>the loaded openapi specification</p> Source code in <code>rs_server_frontend/main.py</code> <pre><code>@staticmethod\ndef load_openapi_spec() -&gt; dict:\n    \"\"\"Load the openapi specification.\n\n    The openapi is loaded from a json file.\n    This json file location is given by the environment variable RSPY_OPENAPI_FILE.\n\n    An IOError is raised in case of errors during the file reading.\n    A ValueError is raised in case of errors during the json parsing.\n\n    Returns:\n        the loaded openapi specification\n\n    \"\"\"\n    openapi_location = os.getenv(\"RSPY_OPENAPI_FILE\", \"\")\n    try:\n        with open(openapi_location, \"r\", encoding=\"utf-8\") as file:\n            return json.load(file)\n    except (FileNotFoundError, IOError) as e:\n        raise type(e)(\n            f\"openapi spec was not found at {openapi_location!r}. \"\n            \"Is the 'RSPY_OPENAPI_FILE' environment variable correctly set ?\",\n        ) from e\n    except ValueError as e:\n        raise ValueError(\n            f\"openapi spec was found at {openapi_location!r} but the file is not valid.\",\n        ) from e\n</code></pre>"},{"location":"frontend/#rs_server_frontend.main.FrontendFailed","title":"<code>FrontendFailed</code>","text":"<p>               Bases: <code>BaseException</code></p> <p>Exception raised if the frontend initialization failed.</p> Source code in <code>rs_server_frontend/main.py</code> <pre><code>class FrontendFailed(BaseException):\n    \"\"\"Exception raised if the frontend initialization failed.\"\"\"\n</code></pre>"},{"location":"frontend/#rs_server_frontend.main.HealthSchema","title":"<code>HealthSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Health status flag.</p> Source code in <code>rs_server_frontend/main.py</code> <pre><code>class HealthSchema(BaseModel):\n    \"\"\"Health status flag.\"\"\"\n\n    healthy: bool\n</code></pre>"},{"location":"frontend/#rs_server_frontend.main.start_app","title":"<code>start_app()</code>","text":"<p>Start the starlette app.</p> <p>Factory function that starts the application.</p> <p>Returns:</p> Type Description <code>FastAPI</code> <p>the initialized application</p> Source code in <code>rs_server_frontend/main.py</code> <pre><code>def start_app() -&gt; FastAPI:\n    \"\"\"Start the starlette app.\n\n    Factory function that starts the application.\n\n    Returns:\n        the initialized application\n\n    \"\"\"\n    return Frontend().app\n</code></pre>"}]}